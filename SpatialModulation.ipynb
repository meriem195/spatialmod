{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpatialModulation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdktJBQELFl_",
        "colab_type": "text"
      },
      "source": [
        "# Spatial Modulation , A deep learning based approach for maximising channel capacity for a Massive MIMO system for 5G communication application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ctVLoPfIu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYhMg7VwfJ3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f650723-beec-46b5-d6ca-fe13eb3e350d"
      },
      "source": [
        "# dataset \n",
        "dataset_dir='/content/complete_dataset.csv'\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,csv_path):\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.col_Names=[\"Bits Number\",\"no of antennas\",\"labels\"]\n",
        "        df=pd.read_csv(csv_path,names=self.col_Names)\n",
        "        self.labels = np.asarray(df.iloc[:,0])\n",
        "        self.inputdata =np.asarray(df.iloc[:,1])\n",
        "        self.inputdata1 =np.asarray(df.iloc[:,2])\n",
        "        self.data_len=len(df.index)\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.data_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        data=self.inputdata[index]\n",
        "        data1=self.inputdata1[index]\n",
        "        data=np.array([data,data1])\n",
        "        label =np.array(self.labels[index])\n",
        "        return torch.from_numpy(data),torch.from_numpy(label)\n",
        "\n",
        "\n",
        "dataset=CustomDataset(dataset_dir)\n",
        "print(dataset.__getitem__(9)[0].size())\n",
        "\n",
        "#print(dataframe.head())new_a = a[np.newaxis, :]\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset,[train_size, test_size])\n",
        "\n",
        "#print(train_dataset[0])\n",
        "\n"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHvZozkTLP-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "d852803b-7315-4327-ecfe-2c664c291173"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = 2\n",
        "output_size=1\n",
        "hidden_size = 20\n",
        "num_classes = 1\n",
        "num_epochs = 1500\n",
        "batch_size = 50\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class Spatialmodulation(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Spatialmodulation, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size,hidden_size) \n",
        "        self.bc1 = nn.BatchNorm1d(hidden_size)\n",
        "        #self.drp1= nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size,hidden_size)  \n",
        "        self.bc2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size,output_size)  \n",
        "        self.bc3 = nn.BatchNorm1d(output_size)\n",
        "       # self.drp2= nn.Dropout(0.5)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(output_size,num_classes) \n",
        "        self.relu4 = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.relu3(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.relu4(out)\n",
        "        return out\n",
        "\n",
        "model = Spatialmodulation(input_size, hidden_size, num_classes)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spatialmodulation(\n",
            "  (fc1): Linear(in_features=2, out_features=20, bias=True)\n",
            "  (bc1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
            "  (bc2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=20, out_features=1, bias=True)\n",
            "  (bc3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu3): ReLU()\n",
            "  (fc4): Linear(in_features=1, out_features=1, bias=True)\n",
            "  (relu4): ReLU()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wsFfk617QeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijhAWT0vY-8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JDB8PiiLbEJ",
        "colab_type": "code",
        "outputId": "c6f27904-a589-4daa-a380-4744319ab193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data,labels) in enumerate(train_loader):        \n",
        "        # Forward pass\n",
        "        labels=labels[:,np.newaxis]\n",
        "        outputs = model(data.float())\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i+1) % batch_size == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for (data,labels) in test_loader:\n",
        "        #data=transforms.ToTensor(data)\n",
        "        outputs = model(data.float())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/1500], Step [50/148], Loss: 26282.2832\n",
            "Epoch [1/1500], Step [100/148], Loss: 27574.8418\n",
            "Epoch [2/1500], Step [50/148], Loss: 18369.5898\n",
            "Epoch [2/1500], Step [100/148], Loss: 27492.1484\n",
            "Epoch [3/1500], Step [50/148], Loss: 16613.9043\n",
            "Epoch [3/1500], Step [100/148], Loss: 13530.2471\n",
            "Epoch [4/1500], Step [50/148], Loss: 25170.1992\n",
            "Epoch [4/1500], Step [100/148], Loss: 20311.0195\n",
            "Epoch [5/1500], Step [50/148], Loss: 15614.0820\n",
            "Epoch [5/1500], Step [100/148], Loss: 15269.9609\n",
            "Epoch [6/1500], Step [50/148], Loss: 16524.2129\n",
            "Epoch [6/1500], Step [100/148], Loss: 13104.1221\n",
            "Epoch [7/1500], Step [50/148], Loss: 11398.3086\n",
            "Epoch [7/1500], Step [100/148], Loss: 9123.8477\n",
            "Epoch [8/1500], Step [50/148], Loss: 8586.7314\n",
            "Epoch [8/1500], Step [100/148], Loss: 16112.5146\n",
            "Epoch [9/1500], Step [50/148], Loss: 10316.3389\n",
            "Epoch [9/1500], Step [100/148], Loss: 11881.9365\n",
            "Epoch [10/1500], Step [50/148], Loss: 8674.8086\n",
            "Epoch [10/1500], Step [100/148], Loss: 9945.2754\n",
            "Epoch [11/1500], Step [50/148], Loss: 9172.0908\n",
            "Epoch [11/1500], Step [100/148], Loss: 12671.9121\n",
            "Epoch [12/1500], Step [50/148], Loss: 18845.3945\n",
            "Epoch [12/1500], Step [100/148], Loss: 8061.8813\n",
            "Epoch [13/1500], Step [50/148], Loss: 6599.9136\n",
            "Epoch [13/1500], Step [100/148], Loss: 3560.4004\n",
            "Epoch [14/1500], Step [50/148], Loss: 9477.6279\n",
            "Epoch [14/1500], Step [100/148], Loss: 4638.0332\n",
            "Epoch [15/1500], Step [50/148], Loss: 8068.6201\n",
            "Epoch [15/1500], Step [100/148], Loss: 7113.7520\n",
            "Epoch [16/1500], Step [50/148], Loss: 9097.3330\n",
            "Epoch [16/1500], Step [100/148], Loss: 3637.2356\n",
            "Epoch [17/1500], Step [50/148], Loss: 5182.6890\n",
            "Epoch [17/1500], Step [100/148], Loss: 5708.6064\n",
            "Epoch [18/1500], Step [50/148], Loss: 16538.4707\n",
            "Epoch [18/1500], Step [100/148], Loss: 5512.1245\n",
            "Epoch [19/1500], Step [50/148], Loss: 5628.7305\n",
            "Epoch [19/1500], Step [100/148], Loss: 11665.4033\n",
            "Epoch [20/1500], Step [50/148], Loss: 4364.4956\n",
            "Epoch [20/1500], Step [100/148], Loss: 10532.7646\n",
            "Epoch [21/1500], Step [50/148], Loss: 9668.4004\n",
            "Epoch [21/1500], Step [100/148], Loss: 7198.7139\n",
            "Epoch [22/1500], Step [50/148], Loss: 5827.8608\n",
            "Epoch [22/1500], Step [100/148], Loss: 6052.8770\n",
            "Epoch [23/1500], Step [50/148], Loss: 6439.9268\n",
            "Epoch [23/1500], Step [100/148], Loss: 4204.6865\n",
            "Epoch [24/1500], Step [50/148], Loss: 4122.6792\n",
            "Epoch [24/1500], Step [100/148], Loss: 4002.1304\n",
            "Epoch [25/1500], Step [50/148], Loss: 6716.5425\n",
            "Epoch [25/1500], Step [100/148], Loss: 4224.9512\n",
            "Epoch [26/1500], Step [50/148], Loss: 3047.0779\n",
            "Epoch [26/1500], Step [100/148], Loss: 5474.8467\n",
            "Epoch [27/1500], Step [50/148], Loss: 6829.6782\n",
            "Epoch [27/1500], Step [100/148], Loss: 8590.3477\n",
            "Epoch [28/1500], Step [50/148], Loss: 5399.0889\n",
            "Epoch [28/1500], Step [100/148], Loss: 4901.6895\n",
            "Epoch [29/1500], Step [50/148], Loss: 3680.6370\n",
            "Epoch [29/1500], Step [100/148], Loss: 4699.4517\n",
            "Epoch [30/1500], Step [50/148], Loss: 3467.8345\n",
            "Epoch [30/1500], Step [100/148], Loss: 2798.9170\n",
            "Epoch [31/1500], Step [50/148], Loss: 9136.1396\n",
            "Epoch [31/1500], Step [100/148], Loss: 3504.2563\n",
            "Epoch [32/1500], Step [50/148], Loss: 3142.8594\n",
            "Epoch [32/1500], Step [100/148], Loss: 7983.7051\n",
            "Epoch [33/1500], Step [50/148], Loss: 3808.5737\n",
            "Epoch [33/1500], Step [100/148], Loss: 3928.2466\n",
            "Epoch [34/1500], Step [50/148], Loss: 6179.2988\n",
            "Epoch [34/1500], Step [100/148], Loss: 6338.8101\n",
            "Epoch [35/1500], Step [50/148], Loss: 6798.0845\n",
            "Epoch [35/1500], Step [100/148], Loss: 5867.6221\n",
            "Epoch [36/1500], Step [50/148], Loss: 5556.9312\n",
            "Epoch [36/1500], Step [100/148], Loss: 7779.1143\n",
            "Epoch [37/1500], Step [50/148], Loss: 5408.9214\n",
            "Epoch [37/1500], Step [100/148], Loss: 7269.3486\n",
            "Epoch [38/1500], Step [50/148], Loss: 4280.6577\n",
            "Epoch [38/1500], Step [100/148], Loss: 4984.5249\n",
            "Epoch [39/1500], Step [50/148], Loss: 9331.5801\n",
            "Epoch [39/1500], Step [100/148], Loss: 5879.9277\n",
            "Epoch [40/1500], Step [50/148], Loss: 4323.8267\n",
            "Epoch [40/1500], Step [100/148], Loss: 6279.7651\n",
            "Epoch [41/1500], Step [50/148], Loss: 4360.2695\n",
            "Epoch [41/1500], Step [100/148], Loss: 9745.4854\n",
            "Epoch [42/1500], Step [50/148], Loss: 7511.6338\n",
            "Epoch [42/1500], Step [100/148], Loss: 3735.4568\n",
            "Epoch [43/1500], Step [50/148], Loss: 7824.1577\n",
            "Epoch [43/1500], Step [100/148], Loss: 5523.3311\n",
            "Epoch [44/1500], Step [50/148], Loss: 2296.6453\n",
            "Epoch [44/1500], Step [100/148], Loss: 4061.2615\n",
            "Epoch [45/1500], Step [50/148], Loss: 2375.4790\n",
            "Epoch [45/1500], Step [100/148], Loss: 3213.8730\n",
            "Epoch [46/1500], Step [50/148], Loss: 7798.9131\n",
            "Epoch [46/1500], Step [100/148], Loss: 4174.4932\n",
            "Epoch [47/1500], Step [50/148], Loss: 2567.3804\n",
            "Epoch [47/1500], Step [100/148], Loss: 6196.9580\n",
            "Epoch [48/1500], Step [50/148], Loss: 6973.4785\n",
            "Epoch [48/1500], Step [100/148], Loss: 2601.2385\n",
            "Epoch [49/1500], Step [50/148], Loss: 4093.7976\n",
            "Epoch [49/1500], Step [100/148], Loss: 5537.0405\n",
            "Epoch [50/1500], Step [50/148], Loss: 4906.0674\n",
            "Epoch [50/1500], Step [100/148], Loss: 5035.3809\n",
            "Epoch [51/1500], Step [50/148], Loss: 3751.1885\n",
            "Epoch [51/1500], Step [100/148], Loss: 7126.3301\n",
            "Epoch [52/1500], Step [50/148], Loss: 5397.3350\n",
            "Epoch [52/1500], Step [100/148], Loss: 3874.8237\n",
            "Epoch [53/1500], Step [50/148], Loss: 11163.6484\n",
            "Epoch [53/1500], Step [100/148], Loss: 8592.5488\n",
            "Epoch [54/1500], Step [50/148], Loss: 5119.5039\n",
            "Epoch [54/1500], Step [100/148], Loss: 11981.6221\n",
            "Epoch [55/1500], Step [50/148], Loss: 2075.4456\n",
            "Epoch [55/1500], Step [100/148], Loss: 3947.4341\n",
            "Epoch [56/1500], Step [50/148], Loss: 4777.2178\n",
            "Epoch [56/1500], Step [100/148], Loss: 7327.4360\n",
            "Epoch [57/1500], Step [50/148], Loss: 6214.8066\n",
            "Epoch [57/1500], Step [100/148], Loss: 4547.7051\n",
            "Epoch [58/1500], Step [50/148], Loss: 4722.5596\n",
            "Epoch [58/1500], Step [100/148], Loss: 3776.0381\n",
            "Epoch [59/1500], Step [50/148], Loss: 2844.2866\n",
            "Epoch [59/1500], Step [100/148], Loss: 3373.7373\n",
            "Epoch [60/1500], Step [50/148], Loss: 6895.4736\n",
            "Epoch [60/1500], Step [100/148], Loss: 3638.9338\n",
            "Epoch [61/1500], Step [50/148], Loss: 17886.1191\n",
            "Epoch [61/1500], Step [100/148], Loss: 5145.2563\n",
            "Epoch [62/1500], Step [50/148], Loss: 5468.8496\n",
            "Epoch [62/1500], Step [100/148], Loss: 9174.9062\n",
            "Epoch [63/1500], Step [50/148], Loss: 10117.1895\n",
            "Epoch [63/1500], Step [100/148], Loss: 3632.2319\n",
            "Epoch [64/1500], Step [50/148], Loss: 3150.1675\n",
            "Epoch [64/1500], Step [100/148], Loss: 3662.1794\n",
            "Epoch [65/1500], Step [50/148], Loss: 4211.4102\n",
            "Epoch [65/1500], Step [100/148], Loss: 5248.9907\n",
            "Epoch [66/1500], Step [50/148], Loss: 3446.0342\n",
            "Epoch [66/1500], Step [100/148], Loss: 3114.1230\n",
            "Epoch [67/1500], Step [50/148], Loss: 5215.1968\n",
            "Epoch [67/1500], Step [100/148], Loss: 3660.9417\n",
            "Epoch [68/1500], Step [50/148], Loss: 1972.6665\n",
            "Epoch [68/1500], Step [100/148], Loss: 3331.0938\n",
            "Epoch [69/1500], Step [50/148], Loss: 2278.7917\n",
            "Epoch [69/1500], Step [100/148], Loss: 2895.4734\n",
            "Epoch [70/1500], Step [50/148], Loss: 4004.2542\n",
            "Epoch [70/1500], Step [100/148], Loss: 3246.1541\n",
            "Epoch [71/1500], Step [50/148], Loss: 5435.7583\n",
            "Epoch [71/1500], Step [100/148], Loss: 7287.8027\n",
            "Epoch [72/1500], Step [50/148], Loss: 2357.9165\n",
            "Epoch [72/1500], Step [100/148], Loss: 6292.2817\n",
            "Epoch [73/1500], Step [50/148], Loss: 6469.0527\n",
            "Epoch [73/1500], Step [100/148], Loss: 3239.8269\n",
            "Epoch [74/1500], Step [50/148], Loss: 2347.9985\n",
            "Epoch [74/1500], Step [100/148], Loss: 3571.1831\n",
            "Epoch [75/1500], Step [50/148], Loss: 7584.0332\n",
            "Epoch [75/1500], Step [100/148], Loss: 3040.1973\n",
            "Epoch [76/1500], Step [50/148], Loss: 2576.9290\n",
            "Epoch [76/1500], Step [100/148], Loss: 1979.7942\n",
            "Epoch [77/1500], Step [50/148], Loss: 7772.6318\n",
            "Epoch [77/1500], Step [100/148], Loss: 4409.3721\n",
            "Epoch [78/1500], Step [50/148], Loss: 6221.4824\n",
            "Epoch [78/1500], Step [100/148], Loss: 2347.3955\n",
            "Epoch [79/1500], Step [50/148], Loss: 7675.5732\n",
            "Epoch [79/1500], Step [100/148], Loss: 5513.7939\n",
            "Epoch [80/1500], Step [50/148], Loss: 7201.5786\n",
            "Epoch [80/1500], Step [100/148], Loss: 5513.6504\n",
            "Epoch [81/1500], Step [50/148], Loss: 2950.0903\n",
            "Epoch [81/1500], Step [100/148], Loss: 5609.6812\n",
            "Epoch [82/1500], Step [50/148], Loss: 3891.6448\n",
            "Epoch [82/1500], Step [100/148], Loss: 5232.7441\n",
            "Epoch [83/1500], Step [50/148], Loss: 7332.9575\n",
            "Epoch [83/1500], Step [100/148], Loss: 3556.1531\n",
            "Epoch [84/1500], Step [50/148], Loss: 5935.3364\n",
            "Epoch [84/1500], Step [100/148], Loss: 3082.1665\n",
            "Epoch [85/1500], Step [50/148], Loss: 3379.0046\n",
            "Epoch [85/1500], Step [100/148], Loss: 7285.9043\n",
            "Epoch [86/1500], Step [50/148], Loss: 8442.4697\n",
            "Epoch [86/1500], Step [100/148], Loss: 2170.9460\n",
            "Epoch [87/1500], Step [50/148], Loss: 3000.2380\n",
            "Epoch [87/1500], Step [100/148], Loss: 2235.9875\n",
            "Epoch [88/1500], Step [50/148], Loss: 6466.9146\n",
            "Epoch [88/1500], Step [100/148], Loss: 5312.0073\n",
            "Epoch [89/1500], Step [50/148], Loss: 3818.9338\n",
            "Epoch [89/1500], Step [100/148], Loss: 1640.7527\n",
            "Epoch [90/1500], Step [50/148], Loss: 11856.7402\n",
            "Epoch [90/1500], Step [100/148], Loss: 6192.7124\n",
            "Epoch [91/1500], Step [50/148], Loss: 5731.3867\n",
            "Epoch [91/1500], Step [100/148], Loss: 2618.2236\n",
            "Epoch [92/1500], Step [50/148], Loss: 11648.7451\n",
            "Epoch [92/1500], Step [100/148], Loss: 3647.2476\n",
            "Epoch [93/1500], Step [50/148], Loss: 1860.6980\n",
            "Epoch [93/1500], Step [100/148], Loss: 6468.2461\n",
            "Epoch [94/1500], Step [50/148], Loss: 4683.2881\n",
            "Epoch [94/1500], Step [100/148], Loss: 4805.8555\n",
            "Epoch [95/1500], Step [50/148], Loss: 4332.2783\n",
            "Epoch [95/1500], Step [100/148], Loss: 4458.5396\n",
            "Epoch [96/1500], Step [50/148], Loss: 9973.4883\n",
            "Epoch [96/1500], Step [100/148], Loss: 1918.5964\n",
            "Epoch [97/1500], Step [50/148], Loss: 3039.9690\n",
            "Epoch [97/1500], Step [100/148], Loss: 3680.2649\n",
            "Epoch [98/1500], Step [50/148], Loss: 12667.1689\n",
            "Epoch [98/1500], Step [100/148], Loss: 4599.4136\n",
            "Epoch [99/1500], Step [50/148], Loss: 2443.0398\n",
            "Epoch [99/1500], Step [100/148], Loss: 3725.0300\n",
            "Epoch [100/1500], Step [50/148], Loss: 5462.4102\n",
            "Epoch [100/1500], Step [100/148], Loss: 5296.4839\n",
            "Epoch [101/1500], Step [50/148], Loss: 8855.2910\n",
            "Epoch [101/1500], Step [100/148], Loss: 6321.1323\n",
            "Epoch [102/1500], Step [50/148], Loss: 3371.4580\n",
            "Epoch [102/1500], Step [100/148], Loss: 5531.4092\n",
            "Epoch [103/1500], Step [50/148], Loss: 3152.4543\n",
            "Epoch [103/1500], Step [100/148], Loss: 4354.4424\n",
            "Epoch [104/1500], Step [50/148], Loss: 1390.0703\n",
            "Epoch [104/1500], Step [100/148], Loss: 1998.2225\n",
            "Epoch [105/1500], Step [50/148], Loss: 5339.9849\n",
            "Epoch [105/1500], Step [100/148], Loss: 3240.1292\n",
            "Epoch [106/1500], Step [50/148], Loss: 2643.2649\n",
            "Epoch [106/1500], Step [100/148], Loss: 6383.6680\n",
            "Epoch [107/1500], Step [50/148], Loss: 5638.5610\n",
            "Epoch [107/1500], Step [100/148], Loss: 3622.2415\n",
            "Epoch [108/1500], Step [50/148], Loss: 3593.7344\n",
            "Epoch [108/1500], Step [100/148], Loss: 4361.8911\n",
            "Epoch [109/1500], Step [50/148], Loss: 1367.1658\n",
            "Epoch [109/1500], Step [100/148], Loss: 5387.6230\n",
            "Epoch [110/1500], Step [50/148], Loss: 5019.5215\n",
            "Epoch [110/1500], Step [100/148], Loss: 12561.6465\n",
            "Epoch [111/1500], Step [50/148], Loss: 4109.6289\n",
            "Epoch [111/1500], Step [100/148], Loss: 10175.8203\n",
            "Epoch [112/1500], Step [50/148], Loss: 4776.5005\n",
            "Epoch [112/1500], Step [100/148], Loss: 880.3753\n",
            "Epoch [113/1500], Step [50/148], Loss: 16050.3945\n",
            "Epoch [113/1500], Step [100/148], Loss: 8435.6670\n",
            "Epoch [114/1500], Step [50/148], Loss: 4416.5371\n",
            "Epoch [114/1500], Step [100/148], Loss: 8309.8076\n",
            "Epoch [115/1500], Step [50/148], Loss: 5669.5171\n",
            "Epoch [115/1500], Step [100/148], Loss: 14826.1279\n",
            "Epoch [116/1500], Step [50/148], Loss: 3678.7344\n",
            "Epoch [116/1500], Step [100/148], Loss: 5007.1396\n",
            "Epoch [117/1500], Step [50/148], Loss: 11314.7686\n",
            "Epoch [117/1500], Step [100/148], Loss: 2928.1128\n",
            "Epoch [118/1500], Step [50/148], Loss: 5532.4795\n",
            "Epoch [118/1500], Step [100/148], Loss: 6625.5005\n",
            "Epoch [119/1500], Step [50/148], Loss: 4856.5957\n",
            "Epoch [119/1500], Step [100/148], Loss: 4973.8574\n",
            "Epoch [120/1500], Step [50/148], Loss: 3378.9485\n",
            "Epoch [120/1500], Step [100/148], Loss: 3737.1399\n",
            "Epoch [121/1500], Step [50/148], Loss: 6956.9512\n",
            "Epoch [121/1500], Step [100/148], Loss: 3736.7842\n",
            "Epoch [122/1500], Step [50/148], Loss: 2309.2393\n",
            "Epoch [122/1500], Step [100/148], Loss: 2197.8701\n",
            "Epoch [123/1500], Step [50/148], Loss: 5440.5483\n",
            "Epoch [123/1500], Step [100/148], Loss: 5375.3867\n",
            "Epoch [124/1500], Step [50/148], Loss: 9421.8232\n",
            "Epoch [124/1500], Step [100/148], Loss: 2893.0146\n",
            "Epoch [125/1500], Step [50/148], Loss: 11288.9297\n",
            "Epoch [125/1500], Step [100/148], Loss: 2822.9138\n",
            "Epoch [126/1500], Step [50/148], Loss: 2845.6086\n",
            "Epoch [126/1500], Step [100/148], Loss: 14253.5000\n",
            "Epoch [127/1500], Step [50/148], Loss: 4854.5254\n",
            "Epoch [127/1500], Step [100/148], Loss: 8002.7764\n",
            "Epoch [128/1500], Step [50/148], Loss: 3348.2412\n",
            "Epoch [128/1500], Step [100/148], Loss: 4422.3955\n",
            "Epoch [129/1500], Step [50/148], Loss: 6803.9707\n",
            "Epoch [129/1500], Step [100/148], Loss: 2745.0454\n",
            "Epoch [130/1500], Step [50/148], Loss: 7116.4937\n",
            "Epoch [130/1500], Step [100/148], Loss: 4737.3374\n",
            "Epoch [131/1500], Step [50/148], Loss: 3702.3674\n",
            "Epoch [131/1500], Step [100/148], Loss: 2927.8481\n",
            "Epoch [132/1500], Step [50/148], Loss: 2387.0444\n",
            "Epoch [132/1500], Step [100/148], Loss: 4576.2549\n",
            "Epoch [133/1500], Step [50/148], Loss: 3344.1179\n",
            "Epoch [133/1500], Step [100/148], Loss: 4279.2432\n",
            "Epoch [134/1500], Step [50/148], Loss: 5373.7207\n",
            "Epoch [134/1500], Step [100/148], Loss: 4230.0962\n",
            "Epoch [135/1500], Step [50/148], Loss: 1087.9943\n",
            "Epoch [135/1500], Step [100/148], Loss: 7768.9482\n",
            "Epoch [136/1500], Step [50/148], Loss: 2744.0840\n",
            "Epoch [136/1500], Step [100/148], Loss: 5258.1401\n",
            "Epoch [137/1500], Step [50/148], Loss: 10436.1084\n",
            "Epoch [137/1500], Step [100/148], Loss: 8588.2061\n",
            "Epoch [138/1500], Step [50/148], Loss: 6691.8301\n",
            "Epoch [138/1500], Step [100/148], Loss: 3379.7805\n",
            "Epoch [139/1500], Step [50/148], Loss: 6915.5469\n",
            "Epoch [139/1500], Step [100/148], Loss: 3280.2021\n",
            "Epoch [140/1500], Step [50/148], Loss: 4409.7617\n",
            "Epoch [140/1500], Step [100/148], Loss: 4639.2754\n",
            "Epoch [141/1500], Step [50/148], Loss: 3923.8467\n",
            "Epoch [141/1500], Step [100/148], Loss: 3618.2600\n",
            "Epoch [142/1500], Step [50/148], Loss: 4312.7280\n",
            "Epoch [142/1500], Step [100/148], Loss: 4063.3508\n",
            "Epoch [143/1500], Step [50/148], Loss: 5217.3428\n",
            "Epoch [143/1500], Step [100/148], Loss: 4356.8901\n",
            "Epoch [144/1500], Step [50/148], Loss: 5195.2793\n",
            "Epoch [144/1500], Step [100/148], Loss: 5589.1289\n",
            "Epoch [145/1500], Step [50/148], Loss: 3316.0112\n",
            "Epoch [145/1500], Step [100/148], Loss: 3287.7498\n",
            "Epoch [146/1500], Step [50/148], Loss: 2796.1106\n",
            "Epoch [146/1500], Step [100/148], Loss: 3846.1394\n",
            "Epoch [147/1500], Step [50/148], Loss: 2037.4352\n",
            "Epoch [147/1500], Step [100/148], Loss: 3550.8015\n",
            "Epoch [148/1500], Step [50/148], Loss: 3247.7112\n",
            "Epoch [148/1500], Step [100/148], Loss: 831.2556\n",
            "Epoch [149/1500], Step [50/148], Loss: 12747.9111\n",
            "Epoch [149/1500], Step [100/148], Loss: 2511.0320\n",
            "Epoch [150/1500], Step [50/148], Loss: 7504.2344\n",
            "Epoch [150/1500], Step [100/148], Loss: 2580.4136\n",
            "Epoch [151/1500], Step [50/148], Loss: 2132.0596\n",
            "Epoch [151/1500], Step [100/148], Loss: 3422.4929\n",
            "Epoch [152/1500], Step [50/148], Loss: 6203.3477\n",
            "Epoch [152/1500], Step [100/148], Loss: 4343.5771\n",
            "Epoch [153/1500], Step [50/148], Loss: 1512.8533\n",
            "Epoch [153/1500], Step [100/148], Loss: 3353.9243\n",
            "Epoch [154/1500], Step [50/148], Loss: 3838.0618\n",
            "Epoch [154/1500], Step [100/148], Loss: 3816.5540\n",
            "Epoch [155/1500], Step [50/148], Loss: 3413.3467\n",
            "Epoch [155/1500], Step [100/148], Loss: 2809.9399\n",
            "Epoch [156/1500], Step [50/148], Loss: 2131.0422\n",
            "Epoch [156/1500], Step [100/148], Loss: 7432.3936\n",
            "Epoch [157/1500], Step [50/148], Loss: 4342.5210\n",
            "Epoch [157/1500], Step [100/148], Loss: 15886.6875\n",
            "Epoch [158/1500], Step [50/148], Loss: 5166.7656\n",
            "Epoch [158/1500], Step [100/148], Loss: 5814.2344\n",
            "Epoch [159/1500], Step [50/148], Loss: 3560.4563\n",
            "Epoch [159/1500], Step [100/148], Loss: 4559.2021\n",
            "Epoch [160/1500], Step [50/148], Loss: 5202.2256\n",
            "Epoch [160/1500], Step [100/148], Loss: 3653.7063\n",
            "Epoch [161/1500], Step [50/148], Loss: 5284.1133\n",
            "Epoch [161/1500], Step [100/148], Loss: 6438.6318\n",
            "Epoch [162/1500], Step [50/148], Loss: 3374.4297\n",
            "Epoch [162/1500], Step [100/148], Loss: 4544.1714\n",
            "Epoch [163/1500], Step [50/148], Loss: 4576.6421\n",
            "Epoch [163/1500], Step [100/148], Loss: 4535.3364\n",
            "Epoch [164/1500], Step [50/148], Loss: 10263.0879\n",
            "Epoch [164/1500], Step [100/148], Loss: 4220.7549\n",
            "Epoch [165/1500], Step [50/148], Loss: 1845.0110\n",
            "Epoch [165/1500], Step [100/148], Loss: 4349.8579\n",
            "Epoch [166/1500], Step [50/148], Loss: 3793.1213\n",
            "Epoch [166/1500], Step [100/148], Loss: 2291.0103\n",
            "Epoch [167/1500], Step [50/148], Loss: 6194.5962\n",
            "Epoch [167/1500], Step [100/148], Loss: 2895.5818\n",
            "Epoch [168/1500], Step [50/148], Loss: 6204.2910\n",
            "Epoch [168/1500], Step [100/148], Loss: 3316.0322\n",
            "Epoch [169/1500], Step [50/148], Loss: 6890.4380\n",
            "Epoch [169/1500], Step [100/148], Loss: 2346.8506\n",
            "Epoch [170/1500], Step [50/148], Loss: 4789.5371\n",
            "Epoch [170/1500], Step [100/148], Loss: 6689.8765\n",
            "Epoch [171/1500], Step [50/148], Loss: 3151.2278\n",
            "Epoch [171/1500], Step [100/148], Loss: 3581.5654\n",
            "Epoch [172/1500], Step [50/148], Loss: 3186.2200\n",
            "Epoch [172/1500], Step [100/148], Loss: 10032.8799\n",
            "Epoch [173/1500], Step [50/148], Loss: 4630.3711\n",
            "Epoch [173/1500], Step [100/148], Loss: 3907.1572\n",
            "Epoch [174/1500], Step [50/148], Loss: 10518.9238\n",
            "Epoch [174/1500], Step [100/148], Loss: 4604.9468\n",
            "Epoch [175/1500], Step [50/148], Loss: 7834.6206\n",
            "Epoch [175/1500], Step [100/148], Loss: 2485.5347\n",
            "Epoch [176/1500], Step [50/148], Loss: 1489.1218\n",
            "Epoch [176/1500], Step [100/148], Loss: 9158.1104\n",
            "Epoch [177/1500], Step [50/148], Loss: 4074.5762\n",
            "Epoch [177/1500], Step [100/148], Loss: 2668.0862\n",
            "Epoch [178/1500], Step [50/148], Loss: 2948.1672\n",
            "Epoch [178/1500], Step [100/148], Loss: 4360.5010\n",
            "Epoch [179/1500], Step [50/148], Loss: 4954.9272\n",
            "Epoch [179/1500], Step [100/148], Loss: 4485.0342\n",
            "Epoch [180/1500], Step [50/148], Loss: 4201.5205\n",
            "Epoch [180/1500], Step [100/148], Loss: 1780.8059\n",
            "Epoch [181/1500], Step [50/148], Loss: 3782.1028\n",
            "Epoch [181/1500], Step [100/148], Loss: 3350.2234\n",
            "Epoch [182/1500], Step [50/148], Loss: 2594.8452\n",
            "Epoch [182/1500], Step [100/148], Loss: 4515.6738\n",
            "Epoch [183/1500], Step [50/148], Loss: 2069.2732\n",
            "Epoch [183/1500], Step [100/148], Loss: 4966.5762\n",
            "Epoch [184/1500], Step [50/148], Loss: 4068.9465\n",
            "Epoch [184/1500], Step [100/148], Loss: 1389.0837\n",
            "Epoch [185/1500], Step [50/148], Loss: 2083.4829\n",
            "Epoch [185/1500], Step [100/148], Loss: 7901.8438\n",
            "Epoch [186/1500], Step [50/148], Loss: 3104.1860\n",
            "Epoch [186/1500], Step [100/148], Loss: 1843.0664\n",
            "Epoch [187/1500], Step [50/148], Loss: 4824.8413\n",
            "Epoch [187/1500], Step [100/148], Loss: 1951.2328\n",
            "Epoch [188/1500], Step [50/148], Loss: 4223.7979\n",
            "Epoch [188/1500], Step [100/148], Loss: 15556.1699\n",
            "Epoch [189/1500], Step [50/148], Loss: 8193.3115\n",
            "Epoch [189/1500], Step [100/148], Loss: 2178.1729\n",
            "Epoch [190/1500], Step [50/148], Loss: 4082.4456\n",
            "Epoch [190/1500], Step [100/148], Loss: 4927.1504\n",
            "Epoch [191/1500], Step [50/148], Loss: 4507.5566\n",
            "Epoch [191/1500], Step [100/148], Loss: 7250.7598\n",
            "Epoch [192/1500], Step [50/148], Loss: 1811.9414\n",
            "Epoch [192/1500], Step [100/148], Loss: 2950.8369\n",
            "Epoch [193/1500], Step [50/148], Loss: 3354.5703\n",
            "Epoch [193/1500], Step [100/148], Loss: 6162.5098\n",
            "Epoch [194/1500], Step [50/148], Loss: 5311.0361\n",
            "Epoch [194/1500], Step [100/148], Loss: 3103.6079\n",
            "Epoch [195/1500], Step [50/148], Loss: 9630.4746\n",
            "Epoch [195/1500], Step [100/148], Loss: 3400.9172\n",
            "Epoch [196/1500], Step [50/148], Loss: 3369.5159\n",
            "Epoch [196/1500], Step [100/148], Loss: 3459.8840\n",
            "Epoch [197/1500], Step [50/148], Loss: 2570.3884\n",
            "Epoch [197/1500], Step [100/148], Loss: 2950.0293\n",
            "Epoch [198/1500], Step [50/148], Loss: 3039.2224\n",
            "Epoch [198/1500], Step [100/148], Loss: 2552.6306\n",
            "Epoch [199/1500], Step [50/148], Loss: 6345.0605\n",
            "Epoch [199/1500], Step [100/148], Loss: 8278.6260\n",
            "Epoch [200/1500], Step [50/148], Loss: 10776.4258\n",
            "Epoch [200/1500], Step [100/148], Loss: 2934.3203\n",
            "Epoch [201/1500], Step [50/148], Loss: 2471.3923\n",
            "Epoch [201/1500], Step [100/148], Loss: 2788.4875\n",
            "Epoch [202/1500], Step [50/148], Loss: 3545.7622\n",
            "Epoch [202/1500], Step [100/148], Loss: 2465.8296\n",
            "Epoch [203/1500], Step [50/148], Loss: 3914.7141\n",
            "Epoch [203/1500], Step [100/148], Loss: 10039.5410\n",
            "Epoch [204/1500], Step [50/148], Loss: 8381.5195\n",
            "Epoch [204/1500], Step [100/148], Loss: 3289.5398\n",
            "Epoch [205/1500], Step [50/148], Loss: 2381.2344\n",
            "Epoch [205/1500], Step [100/148], Loss: 4980.4829\n",
            "Epoch [206/1500], Step [50/148], Loss: 1418.5757\n",
            "Epoch [206/1500], Step [100/148], Loss: 3057.2588\n",
            "Epoch [207/1500], Step [50/148], Loss: 4207.0312\n",
            "Epoch [207/1500], Step [100/148], Loss: 3223.3828\n",
            "Epoch [208/1500], Step [50/148], Loss: 8163.5049\n",
            "Epoch [208/1500], Step [100/148], Loss: 3537.0950\n",
            "Epoch [209/1500], Step [50/148], Loss: 8748.4404\n",
            "Epoch [209/1500], Step [100/148], Loss: 4003.5391\n",
            "Epoch [210/1500], Step [50/148], Loss: 2696.1169\n",
            "Epoch [210/1500], Step [100/148], Loss: 14526.0928\n",
            "Epoch [211/1500], Step [50/148], Loss: 2615.3264\n",
            "Epoch [211/1500], Step [100/148], Loss: 1968.4664\n",
            "Epoch [212/1500], Step [50/148], Loss: 2180.1201\n",
            "Epoch [212/1500], Step [100/148], Loss: 3937.5596\n",
            "Epoch [213/1500], Step [50/148], Loss: 14455.4424\n",
            "Epoch [213/1500], Step [100/148], Loss: 2089.7742\n",
            "Epoch [214/1500], Step [50/148], Loss: 3817.6711\n",
            "Epoch [214/1500], Step [100/148], Loss: 1509.7667\n",
            "Epoch [215/1500], Step [50/148], Loss: 1697.1825\n",
            "Epoch [215/1500], Step [100/148], Loss: 2751.4082\n",
            "Epoch [216/1500], Step [50/148], Loss: 7342.1255\n",
            "Epoch [216/1500], Step [100/148], Loss: 3463.2805\n",
            "Epoch [217/1500], Step [50/148], Loss: 7103.0342\n",
            "Epoch [217/1500], Step [100/148], Loss: 3370.9968\n",
            "Epoch [218/1500], Step [50/148], Loss: 4459.9434\n",
            "Epoch [218/1500], Step [100/148], Loss: 2549.4163\n",
            "Epoch [219/1500], Step [50/148], Loss: 31350.0742\n",
            "Epoch [219/1500], Step [100/148], Loss: 7878.5986\n",
            "Epoch [220/1500], Step [50/148], Loss: 2422.2219\n",
            "Epoch [220/1500], Step [100/148], Loss: 6637.7808\n",
            "Epoch [221/1500], Step [50/148], Loss: 13966.3584\n",
            "Epoch [221/1500], Step [100/148], Loss: 3262.1072\n",
            "Epoch [222/1500], Step [50/148], Loss: 2330.7061\n",
            "Epoch [222/1500], Step [100/148], Loss: 9776.0762\n",
            "Epoch [223/1500], Step [50/148], Loss: 1184.1572\n",
            "Epoch [223/1500], Step [100/148], Loss: 2882.3306\n",
            "Epoch [224/1500], Step [50/148], Loss: 2644.1284\n",
            "Epoch [224/1500], Step [100/148], Loss: 4587.2402\n",
            "Epoch [225/1500], Step [50/148], Loss: 4872.5811\n",
            "Epoch [225/1500], Step [100/148], Loss: 5338.9985\n",
            "Epoch [226/1500], Step [50/148], Loss: 12242.6729\n",
            "Epoch [226/1500], Step [100/148], Loss: 5716.8486\n",
            "Epoch [227/1500], Step [50/148], Loss: 3143.1160\n",
            "Epoch [227/1500], Step [100/148], Loss: 2421.3164\n",
            "Epoch [228/1500], Step [50/148], Loss: 6388.8149\n",
            "Epoch [228/1500], Step [100/148], Loss: 1939.1356\n",
            "Epoch [229/1500], Step [50/148], Loss: 4914.5874\n",
            "Epoch [229/1500], Step [100/148], Loss: 2258.1855\n",
            "Epoch [230/1500], Step [50/148], Loss: 2998.7932\n",
            "Epoch [230/1500], Step [100/148], Loss: 3369.0593\n",
            "Epoch [231/1500], Step [50/148], Loss: 4540.3833\n",
            "Epoch [231/1500], Step [100/148], Loss: 6302.5176\n",
            "Epoch [232/1500], Step [50/148], Loss: 3247.9729\n",
            "Epoch [232/1500], Step [100/148], Loss: 2893.8970\n",
            "Epoch [233/1500], Step [50/148], Loss: 6222.8154\n",
            "Epoch [233/1500], Step [100/148], Loss: 3981.0378\n",
            "Epoch [234/1500], Step [50/148], Loss: 3366.7485\n",
            "Epoch [234/1500], Step [100/148], Loss: 5442.7905\n",
            "Epoch [235/1500], Step [50/148], Loss: 4169.3081\n",
            "Epoch [235/1500], Step [100/148], Loss: 3589.0867\n",
            "Epoch [236/1500], Step [50/148], Loss: 3896.6096\n",
            "Epoch [236/1500], Step [100/148], Loss: 3169.0142\n",
            "Epoch [237/1500], Step [50/148], Loss: 7030.0908\n",
            "Epoch [237/1500], Step [100/148], Loss: 3702.1946\n",
            "Epoch [238/1500], Step [50/148], Loss: 3438.8601\n",
            "Epoch [238/1500], Step [100/148], Loss: 4020.7046\n",
            "Epoch [239/1500], Step [50/148], Loss: 3774.4397\n",
            "Epoch [239/1500], Step [100/148], Loss: 4898.7676\n",
            "Epoch [240/1500], Step [50/148], Loss: 1627.2545\n",
            "Epoch [240/1500], Step [100/148], Loss: 3568.8552\n",
            "Epoch [241/1500], Step [50/148], Loss: 1463.8756\n",
            "Epoch [241/1500], Step [100/148], Loss: 2890.0461\n",
            "Epoch [242/1500], Step [50/148], Loss: 2815.7039\n",
            "Epoch [242/1500], Step [100/148], Loss: 8527.8555\n",
            "Epoch [243/1500], Step [50/148], Loss: 10900.2891\n",
            "Epoch [243/1500], Step [100/148], Loss: 2016.3696\n",
            "Epoch [244/1500], Step [50/148], Loss: 3752.0859\n",
            "Epoch [244/1500], Step [100/148], Loss: 12550.7852\n",
            "Epoch [245/1500], Step [50/148], Loss: 5324.1030\n",
            "Epoch [245/1500], Step [100/148], Loss: 3550.9304\n",
            "Epoch [246/1500], Step [50/148], Loss: 6102.2124\n",
            "Epoch [246/1500], Step [100/148], Loss: 2275.2556\n",
            "Epoch [247/1500], Step [50/148], Loss: 4111.0967\n",
            "Epoch [247/1500], Step [100/148], Loss: 4099.7588\n",
            "Epoch [248/1500], Step [50/148], Loss: 2729.9810\n",
            "Epoch [248/1500], Step [100/148], Loss: 4347.5742\n",
            "Epoch [249/1500], Step [50/148], Loss: 6544.2676\n",
            "Epoch [249/1500], Step [100/148], Loss: 4040.0916\n",
            "Epoch [250/1500], Step [50/148], Loss: 4950.9971\n",
            "Epoch [250/1500], Step [100/148], Loss: 4748.6162\n",
            "Epoch [251/1500], Step [50/148], Loss: 6466.8794\n",
            "Epoch [251/1500], Step [100/148], Loss: 4988.0371\n",
            "Epoch [252/1500], Step [50/148], Loss: 2275.5771\n",
            "Epoch [252/1500], Step [100/148], Loss: 3808.7554\n",
            "Epoch [253/1500], Step [50/148], Loss: 8690.9219\n",
            "Epoch [253/1500], Step [100/148], Loss: 4467.1099\n",
            "Epoch [254/1500], Step [50/148], Loss: 1837.8922\n",
            "Epoch [254/1500], Step [100/148], Loss: 3720.1899\n",
            "Epoch [255/1500], Step [50/148], Loss: 3077.0571\n",
            "Epoch [255/1500], Step [100/148], Loss: 2775.4307\n",
            "Epoch [256/1500], Step [50/148], Loss: 1264.7013\n",
            "Epoch [256/1500], Step [100/148], Loss: 2042.6633\n",
            "Epoch [257/1500], Step [50/148], Loss: 9298.5117\n",
            "Epoch [257/1500], Step [100/148], Loss: 3218.1562\n",
            "Epoch [258/1500], Step [50/148], Loss: 1682.8979\n",
            "Epoch [258/1500], Step [100/148], Loss: 2074.9343\n",
            "Epoch [259/1500], Step [50/148], Loss: 3546.2185\n",
            "Epoch [259/1500], Step [100/148], Loss: 8889.0762\n",
            "Epoch [260/1500], Step [50/148], Loss: 4928.0894\n",
            "Epoch [260/1500], Step [100/148], Loss: 5018.3047\n",
            "Epoch [261/1500], Step [50/148], Loss: 5645.0464\n",
            "Epoch [261/1500], Step [100/148], Loss: 6463.8350\n",
            "Epoch [262/1500], Step [50/148], Loss: 6566.4282\n",
            "Epoch [262/1500], Step [100/148], Loss: 2230.7742\n",
            "Epoch [263/1500], Step [50/148], Loss: 1883.1542\n",
            "Epoch [263/1500], Step [100/148], Loss: 7115.6562\n",
            "Epoch [264/1500], Step [50/148], Loss: 4512.9673\n",
            "Epoch [264/1500], Step [100/148], Loss: 6887.6445\n",
            "Epoch [265/1500], Step [50/148], Loss: 5537.3081\n",
            "Epoch [265/1500], Step [100/148], Loss: 4932.6929\n",
            "Epoch [266/1500], Step [50/148], Loss: 2066.6558\n",
            "Epoch [266/1500], Step [100/148], Loss: 2429.3127\n",
            "Epoch [267/1500], Step [50/148], Loss: 1482.6700\n",
            "Epoch [267/1500], Step [100/148], Loss: 1377.1161\n",
            "Epoch [268/1500], Step [50/148], Loss: 3201.1414\n",
            "Epoch [268/1500], Step [100/148], Loss: 3270.9734\n",
            "Epoch [269/1500], Step [50/148], Loss: 2642.6287\n",
            "Epoch [269/1500], Step [100/148], Loss: 1804.0428\n",
            "Epoch [270/1500], Step [50/148], Loss: 5785.7324\n",
            "Epoch [270/1500], Step [100/148], Loss: 1490.5607\n",
            "Epoch [271/1500], Step [50/148], Loss: 4711.8906\n",
            "Epoch [271/1500], Step [100/148], Loss: 3223.8594\n",
            "Epoch [272/1500], Step [50/148], Loss: 3353.1648\n",
            "Epoch [272/1500], Step [100/148], Loss: 2773.1455\n",
            "Epoch [273/1500], Step [50/148], Loss: 4929.6353\n",
            "Epoch [273/1500], Step [100/148], Loss: 3486.1514\n",
            "Epoch [274/1500], Step [50/148], Loss: 4287.2476\n",
            "Epoch [274/1500], Step [100/148], Loss: 4584.4258\n",
            "Epoch [275/1500], Step [50/148], Loss: 4692.5786\n",
            "Epoch [275/1500], Step [100/148], Loss: 1373.5948\n",
            "Epoch [276/1500], Step [50/148], Loss: 2984.7991\n",
            "Epoch [276/1500], Step [100/148], Loss: 4487.7173\n",
            "Epoch [277/1500], Step [50/148], Loss: 6220.0254\n",
            "Epoch [277/1500], Step [100/148], Loss: 4132.9111\n",
            "Epoch [278/1500], Step [50/148], Loss: 2734.8906\n",
            "Epoch [278/1500], Step [100/148], Loss: 2106.8774\n",
            "Epoch [279/1500], Step [50/148], Loss: 4390.4014\n",
            "Epoch [279/1500], Step [100/148], Loss: 1870.2227\n",
            "Epoch [280/1500], Step [50/148], Loss: 2738.9844\n",
            "Epoch [280/1500], Step [100/148], Loss: 2767.8813\n",
            "Epoch [281/1500], Step [50/148], Loss: 2249.1875\n",
            "Epoch [281/1500], Step [100/148], Loss: 1792.9368\n",
            "Epoch [282/1500], Step [50/148], Loss: 7636.6445\n",
            "Epoch [282/1500], Step [100/148], Loss: 2657.9260\n",
            "Epoch [283/1500], Step [50/148], Loss: 5176.0967\n",
            "Epoch [283/1500], Step [100/148], Loss: 6053.3862\n",
            "Epoch [284/1500], Step [50/148], Loss: 4469.7139\n",
            "Epoch [284/1500], Step [100/148], Loss: 1907.1924\n",
            "Epoch [285/1500], Step [50/148], Loss: 4075.2202\n",
            "Epoch [285/1500], Step [100/148], Loss: 14878.4326\n",
            "Epoch [286/1500], Step [50/148], Loss: 2927.9338\n",
            "Epoch [286/1500], Step [100/148], Loss: 7449.0864\n",
            "Epoch [287/1500], Step [50/148], Loss: 3982.4382\n",
            "Epoch [287/1500], Step [100/148], Loss: 7327.6787\n",
            "Epoch [288/1500], Step [50/148], Loss: 4844.2510\n",
            "Epoch [288/1500], Step [100/148], Loss: 1931.8236\n",
            "Epoch [289/1500], Step [50/148], Loss: 2037.6600\n",
            "Epoch [289/1500], Step [100/148], Loss: 4640.1997\n",
            "Epoch [290/1500], Step [50/148], Loss: 2941.7476\n",
            "Epoch [290/1500], Step [100/148], Loss: 5159.0186\n",
            "Epoch [291/1500], Step [50/148], Loss: 11585.8408\n",
            "Epoch [291/1500], Step [100/148], Loss: 2797.9912\n",
            "Epoch [292/1500], Step [50/148], Loss: 4425.1914\n",
            "Epoch [292/1500], Step [100/148], Loss: 3073.9094\n",
            "Epoch [293/1500], Step [50/148], Loss: 5005.8320\n",
            "Epoch [293/1500], Step [100/148], Loss: 6151.1318\n",
            "Epoch [294/1500], Step [50/148], Loss: 6693.5039\n",
            "Epoch [294/1500], Step [100/148], Loss: 3502.0232\n",
            "Epoch [295/1500], Step [50/148], Loss: 5226.9277\n",
            "Epoch [295/1500], Step [100/148], Loss: 1882.4126\n",
            "Epoch [296/1500], Step [50/148], Loss: 2271.9543\n",
            "Epoch [296/1500], Step [100/148], Loss: 1077.1820\n",
            "Epoch [297/1500], Step [50/148], Loss: 3021.1509\n",
            "Epoch [297/1500], Step [100/148], Loss: 2087.8274\n",
            "Epoch [298/1500], Step [50/148], Loss: 2731.3269\n",
            "Epoch [298/1500], Step [100/148], Loss: 4186.7910\n",
            "Epoch [299/1500], Step [50/148], Loss: 2721.6934\n",
            "Epoch [299/1500], Step [100/148], Loss: 6728.7070\n",
            "Epoch [300/1500], Step [50/148], Loss: 3097.0107\n",
            "Epoch [300/1500], Step [100/148], Loss: 5312.7896\n",
            "Epoch [301/1500], Step [50/148], Loss: 4601.6035\n",
            "Epoch [301/1500], Step [100/148], Loss: 5103.8296\n",
            "Epoch [302/1500], Step [50/148], Loss: 13672.7314\n",
            "Epoch [302/1500], Step [100/148], Loss: 5358.0444\n",
            "Epoch [303/1500], Step [50/148], Loss: 3842.4341\n",
            "Epoch [303/1500], Step [100/148], Loss: 3109.0457\n",
            "Epoch [304/1500], Step [50/148], Loss: 18286.4629\n",
            "Epoch [304/1500], Step [100/148], Loss: 3188.8704\n",
            "Epoch [305/1500], Step [50/148], Loss: 4223.5098\n",
            "Epoch [305/1500], Step [100/148], Loss: 2504.2744\n",
            "Epoch [306/1500], Step [50/148], Loss: 5863.9170\n",
            "Epoch [306/1500], Step [100/148], Loss: 4456.3877\n",
            "Epoch [307/1500], Step [50/148], Loss: 3873.3069\n",
            "Epoch [307/1500], Step [100/148], Loss: 3373.6968\n",
            "Epoch [308/1500], Step [50/148], Loss: 10326.2900\n",
            "Epoch [308/1500], Step [100/148], Loss: 3803.5791\n",
            "Epoch [309/1500], Step [50/148], Loss: 7077.3193\n",
            "Epoch [309/1500], Step [100/148], Loss: 5288.8970\n",
            "Epoch [310/1500], Step [50/148], Loss: 3307.0828\n",
            "Epoch [310/1500], Step [100/148], Loss: 4245.9219\n",
            "Epoch [311/1500], Step [50/148], Loss: 11055.4629\n",
            "Epoch [311/1500], Step [100/148], Loss: 1615.4812\n",
            "Epoch [312/1500], Step [50/148], Loss: 3614.8503\n",
            "Epoch [312/1500], Step [100/148], Loss: 3399.3816\n",
            "Epoch [313/1500], Step [50/148], Loss: 2223.8313\n",
            "Epoch [313/1500], Step [100/148], Loss: 7585.9282\n",
            "Epoch [314/1500], Step [50/148], Loss: 3801.2642\n",
            "Epoch [314/1500], Step [100/148], Loss: 1096.8004\n",
            "Epoch [315/1500], Step [50/148], Loss: 5438.1816\n",
            "Epoch [315/1500], Step [100/148], Loss: 4309.4258\n",
            "Epoch [316/1500], Step [50/148], Loss: 3193.5950\n",
            "Epoch [316/1500], Step [100/148], Loss: 4700.6685\n",
            "Epoch [317/1500], Step [50/148], Loss: 2832.6274\n",
            "Epoch [317/1500], Step [100/148], Loss: 1933.3564\n",
            "Epoch [318/1500], Step [50/148], Loss: 6691.1206\n",
            "Epoch [318/1500], Step [100/148], Loss: 10110.7930\n",
            "Epoch [319/1500], Step [50/148], Loss: 3561.4819\n",
            "Epoch [319/1500], Step [100/148], Loss: 6842.3687\n",
            "Epoch [320/1500], Step [50/148], Loss: 4217.9976\n",
            "Epoch [320/1500], Step [100/148], Loss: 2641.8469\n",
            "Epoch [321/1500], Step [50/148], Loss: 2427.7688\n",
            "Epoch [321/1500], Step [100/148], Loss: 3244.1162\n",
            "Epoch [322/1500], Step [50/148], Loss: 5031.0923\n",
            "Epoch [322/1500], Step [100/148], Loss: 2968.9255\n",
            "Epoch [323/1500], Step [50/148], Loss: 3643.8740\n",
            "Epoch [323/1500], Step [100/148], Loss: 3673.1362\n",
            "Epoch [324/1500], Step [50/148], Loss: 4856.0215\n",
            "Epoch [324/1500], Step [100/148], Loss: 2454.8240\n",
            "Epoch [325/1500], Step [50/148], Loss: 2014.6925\n",
            "Epoch [325/1500], Step [100/148], Loss: 4246.8472\n",
            "Epoch [326/1500], Step [50/148], Loss: 6294.0840\n",
            "Epoch [326/1500], Step [100/148], Loss: 3973.5984\n",
            "Epoch [327/1500], Step [50/148], Loss: 2199.7078\n",
            "Epoch [327/1500], Step [100/148], Loss: 1421.8601\n",
            "Epoch [328/1500], Step [50/148], Loss: 1342.2177\n",
            "Epoch [328/1500], Step [100/148], Loss: 4332.0762\n",
            "Epoch [329/1500], Step [50/148], Loss: 3367.4868\n",
            "Epoch [329/1500], Step [100/148], Loss: 4347.1279\n",
            "Epoch [330/1500], Step [50/148], Loss: 4306.9883\n",
            "Epoch [330/1500], Step [100/148], Loss: 2785.9131\n",
            "Epoch [331/1500], Step [50/148], Loss: 5763.7896\n",
            "Epoch [331/1500], Step [100/148], Loss: 6395.1494\n",
            "Epoch [332/1500], Step [50/148], Loss: 3362.0908\n",
            "Epoch [332/1500], Step [100/148], Loss: 2158.1479\n",
            "Epoch [333/1500], Step [50/148], Loss: 3243.5081\n",
            "Epoch [333/1500], Step [100/148], Loss: 2034.9854\n",
            "Epoch [334/1500], Step [50/148], Loss: 5512.6489\n",
            "Epoch [334/1500], Step [100/148], Loss: 1854.0231\n",
            "Epoch [335/1500], Step [50/148], Loss: 4480.0459\n",
            "Epoch [335/1500], Step [100/148], Loss: 3275.7834\n",
            "Epoch [336/1500], Step [50/148], Loss: 4068.0381\n",
            "Epoch [336/1500], Step [100/148], Loss: 3765.2498\n",
            "Epoch [337/1500], Step [50/148], Loss: 8637.9834\n",
            "Epoch [337/1500], Step [100/148], Loss: 3904.4568\n",
            "Epoch [338/1500], Step [50/148], Loss: 1828.9950\n",
            "Epoch [338/1500], Step [100/148], Loss: 2750.6030\n",
            "Epoch [339/1500], Step [50/148], Loss: 3925.4221\n",
            "Epoch [339/1500], Step [100/148], Loss: 1930.1647\n",
            "Epoch [340/1500], Step [50/148], Loss: 3863.7771\n",
            "Epoch [340/1500], Step [100/148], Loss: 1464.3029\n",
            "Epoch [341/1500], Step [50/148], Loss: 6838.4771\n",
            "Epoch [341/1500], Step [100/148], Loss: 3905.4946\n",
            "Epoch [342/1500], Step [50/148], Loss: 2932.1741\n",
            "Epoch [342/1500], Step [100/148], Loss: 6367.4395\n",
            "Epoch [343/1500], Step [50/148], Loss: 2381.3345\n",
            "Epoch [343/1500], Step [100/148], Loss: 1057.1436\n",
            "Epoch [344/1500], Step [50/148], Loss: 1868.0590\n",
            "Epoch [344/1500], Step [100/148], Loss: 1926.7666\n",
            "Epoch [345/1500], Step [50/148], Loss: 3623.8215\n",
            "Epoch [345/1500], Step [100/148], Loss: 2904.1418\n",
            "Epoch [346/1500], Step [50/148], Loss: 5807.4907\n",
            "Epoch [346/1500], Step [100/148], Loss: 1233.7603\n",
            "Epoch [347/1500], Step [50/148], Loss: 1429.7533\n",
            "Epoch [347/1500], Step [100/148], Loss: 2419.5027\n",
            "Epoch [348/1500], Step [50/148], Loss: 4397.4829\n",
            "Epoch [348/1500], Step [100/148], Loss: 4696.0815\n",
            "Epoch [349/1500], Step [50/148], Loss: 3945.0010\n",
            "Epoch [349/1500], Step [100/148], Loss: 2275.0940\n",
            "Epoch [350/1500], Step [50/148], Loss: 2837.7920\n",
            "Epoch [350/1500], Step [100/148], Loss: 8887.8203\n",
            "Epoch [351/1500], Step [50/148], Loss: 8403.4375\n",
            "Epoch [351/1500], Step [100/148], Loss: 5402.8296\n",
            "Epoch [352/1500], Step [50/148], Loss: 3603.7937\n",
            "Epoch [352/1500], Step [100/148], Loss: 2132.9568\n",
            "Epoch [353/1500], Step [50/148], Loss: 2170.5049\n",
            "Epoch [353/1500], Step [100/148], Loss: 12054.4287\n",
            "Epoch [354/1500], Step [50/148], Loss: 3220.1570\n",
            "Epoch [354/1500], Step [100/148], Loss: 2365.8989\n",
            "Epoch [355/1500], Step [50/148], Loss: 8508.1035\n",
            "Epoch [355/1500], Step [100/148], Loss: 6890.9673\n",
            "Epoch [356/1500], Step [50/148], Loss: 2147.0693\n",
            "Epoch [356/1500], Step [100/148], Loss: 4105.1089\n",
            "Epoch [357/1500], Step [50/148], Loss: 9264.1709\n",
            "Epoch [357/1500], Step [100/148], Loss: 849.3924\n",
            "Epoch [358/1500], Step [50/148], Loss: 1268.7985\n",
            "Epoch [358/1500], Step [100/148], Loss: 4191.7456\n",
            "Epoch [359/1500], Step [50/148], Loss: 7928.0352\n",
            "Epoch [359/1500], Step [100/148], Loss: 3261.7063\n",
            "Epoch [360/1500], Step [50/148], Loss: 1863.4119\n",
            "Epoch [360/1500], Step [100/148], Loss: 4156.6392\n",
            "Epoch [361/1500], Step [50/148], Loss: 5160.8130\n",
            "Epoch [361/1500], Step [100/148], Loss: 2255.9543\n",
            "Epoch [362/1500], Step [50/148], Loss: 4569.5845\n",
            "Epoch [362/1500], Step [100/148], Loss: 7938.7480\n",
            "Epoch [363/1500], Step [50/148], Loss: 2775.9412\n",
            "Epoch [363/1500], Step [100/148], Loss: 8445.5303\n",
            "Epoch [364/1500], Step [50/148], Loss: 2968.0767\n",
            "Epoch [364/1500], Step [100/148], Loss: 4952.6479\n",
            "Epoch [365/1500], Step [50/148], Loss: 4222.6440\n",
            "Epoch [365/1500], Step [100/148], Loss: 2314.3215\n",
            "Epoch [366/1500], Step [50/148], Loss: 2638.4087\n",
            "Epoch [366/1500], Step [100/148], Loss: 2123.5447\n",
            "Epoch [367/1500], Step [50/148], Loss: 2384.2236\n",
            "Epoch [367/1500], Step [100/148], Loss: 4652.7148\n",
            "Epoch [368/1500], Step [50/148], Loss: 5976.0176\n",
            "Epoch [368/1500], Step [100/148], Loss: 11172.3564\n",
            "Epoch [369/1500], Step [50/148], Loss: 2016.7389\n",
            "Epoch [369/1500], Step [100/148], Loss: 4873.9297\n",
            "Epoch [370/1500], Step [50/148], Loss: 2321.7476\n",
            "Epoch [370/1500], Step [100/148], Loss: 3479.8486\n",
            "Epoch [371/1500], Step [50/148], Loss: 5488.4346\n",
            "Epoch [371/1500], Step [100/148], Loss: 4693.5576\n",
            "Epoch [372/1500], Step [50/148], Loss: 1924.4578\n",
            "Epoch [372/1500], Step [100/148], Loss: 5072.6548\n",
            "Epoch [373/1500], Step [50/148], Loss: 4441.1973\n",
            "Epoch [373/1500], Step [100/148], Loss: 3691.1875\n",
            "Epoch [374/1500], Step [50/148], Loss: 2804.6440\n",
            "Epoch [374/1500], Step [100/148], Loss: 4973.7261\n",
            "Epoch [375/1500], Step [50/148], Loss: 3544.4490\n",
            "Epoch [375/1500], Step [100/148], Loss: 5780.5845\n",
            "Epoch [376/1500], Step [50/148], Loss: 3838.9624\n",
            "Epoch [376/1500], Step [100/148], Loss: 9087.2305\n",
            "Epoch [377/1500], Step [50/148], Loss: 5609.0098\n",
            "Epoch [377/1500], Step [100/148], Loss: 6269.6660\n",
            "Epoch [378/1500], Step [50/148], Loss: 3370.1494\n",
            "Epoch [378/1500], Step [100/148], Loss: 4073.7075\n",
            "Epoch [379/1500], Step [50/148], Loss: 2193.8201\n",
            "Epoch [379/1500], Step [100/148], Loss: 2665.3188\n",
            "Epoch [380/1500], Step [50/148], Loss: 3843.2446\n",
            "Epoch [380/1500], Step [100/148], Loss: 1129.9425\n",
            "Epoch [381/1500], Step [50/148], Loss: 3564.3916\n",
            "Epoch [381/1500], Step [100/148], Loss: 5071.5620\n",
            "Epoch [382/1500], Step [50/148], Loss: 2620.5830\n",
            "Epoch [382/1500], Step [100/148], Loss: 5394.8330\n",
            "Epoch [383/1500], Step [50/148], Loss: 6381.0200\n",
            "Epoch [383/1500], Step [100/148], Loss: 4539.4668\n",
            "Epoch [384/1500], Step [50/148], Loss: 5094.3276\n",
            "Epoch [384/1500], Step [100/148], Loss: 2795.1074\n",
            "Epoch [385/1500], Step [50/148], Loss: 5973.2056\n",
            "Epoch [385/1500], Step [100/148], Loss: 3939.3408\n",
            "Epoch [386/1500], Step [50/148], Loss: 5882.9424\n",
            "Epoch [386/1500], Step [100/148], Loss: 3412.3032\n",
            "Epoch [387/1500], Step [50/148], Loss: 7432.2808\n",
            "Epoch [387/1500], Step [100/148], Loss: 4322.5684\n",
            "Epoch [388/1500], Step [50/148], Loss: 4105.1777\n",
            "Epoch [388/1500], Step [100/148], Loss: 2395.1091\n",
            "Epoch [389/1500], Step [50/148], Loss: 2486.8765\n",
            "Epoch [389/1500], Step [100/148], Loss: 4476.0640\n",
            "Epoch [390/1500], Step [50/148], Loss: 4219.9180\n",
            "Epoch [390/1500], Step [100/148], Loss: 12663.3301\n",
            "Epoch [391/1500], Step [50/148], Loss: 2268.5559\n",
            "Epoch [391/1500], Step [100/148], Loss: 914.2549\n",
            "Epoch [392/1500], Step [50/148], Loss: 3995.4517\n",
            "Epoch [392/1500], Step [100/148], Loss: 3065.4265\n",
            "Epoch [393/1500], Step [50/148], Loss: 1341.5414\n",
            "Epoch [393/1500], Step [100/148], Loss: 3096.3416\n",
            "Epoch [394/1500], Step [50/148], Loss: 4244.2290\n",
            "Epoch [394/1500], Step [100/148], Loss: 1963.3165\n",
            "Epoch [395/1500], Step [50/148], Loss: 4621.8193\n",
            "Epoch [395/1500], Step [100/148], Loss: 4697.8623\n",
            "Epoch [396/1500], Step [50/148], Loss: 2698.4551\n",
            "Epoch [396/1500], Step [100/148], Loss: 1880.0172\n",
            "Epoch [397/1500], Step [50/148], Loss: 3786.1074\n",
            "Epoch [397/1500], Step [100/148], Loss: 6309.8589\n",
            "Epoch [398/1500], Step [50/148], Loss: 9612.6445\n",
            "Epoch [398/1500], Step [100/148], Loss: 3470.2810\n",
            "Epoch [399/1500], Step [50/148], Loss: 1649.7664\n",
            "Epoch [399/1500], Step [100/148], Loss: 809.7556\n",
            "Epoch [400/1500], Step [50/148], Loss: 1899.4432\n",
            "Epoch [400/1500], Step [100/148], Loss: 11806.5391\n",
            "Epoch [401/1500], Step [50/148], Loss: 1872.5575\n",
            "Epoch [401/1500], Step [100/148], Loss: 5808.0688\n",
            "Epoch [402/1500], Step [50/148], Loss: 4843.7534\n",
            "Epoch [402/1500], Step [100/148], Loss: 1405.6161\n",
            "Epoch [403/1500], Step [50/148], Loss: 4440.0908\n",
            "Epoch [403/1500], Step [100/148], Loss: 949.0688\n",
            "Epoch [404/1500], Step [50/148], Loss: 3161.4695\n",
            "Epoch [404/1500], Step [100/148], Loss: 2296.7429\n",
            "Epoch [405/1500], Step [50/148], Loss: 3366.9275\n",
            "Epoch [405/1500], Step [100/148], Loss: 3095.0112\n",
            "Epoch [406/1500], Step [50/148], Loss: 1997.5485\n",
            "Epoch [406/1500], Step [100/148], Loss: 5801.1265\n",
            "Epoch [407/1500], Step [50/148], Loss: 3716.5120\n",
            "Epoch [407/1500], Step [100/148], Loss: 8025.5869\n",
            "Epoch [408/1500], Step [50/148], Loss: 1295.9337\n",
            "Epoch [408/1500], Step [100/148], Loss: 2922.3564\n",
            "Epoch [409/1500], Step [50/148], Loss: 2414.5330\n",
            "Epoch [409/1500], Step [100/148], Loss: 1375.0798\n",
            "Epoch [410/1500], Step [50/148], Loss: 5947.1523\n",
            "Epoch [410/1500], Step [100/148], Loss: 3068.7163\n",
            "Epoch [411/1500], Step [50/148], Loss: 4392.0122\n",
            "Epoch [411/1500], Step [100/148], Loss: 2480.1436\n",
            "Epoch [412/1500], Step [50/148], Loss: 2803.7253\n",
            "Epoch [412/1500], Step [100/148], Loss: 4810.6367\n",
            "Epoch [413/1500], Step [50/148], Loss: 3590.4741\n",
            "Epoch [413/1500], Step [100/148], Loss: 3674.9580\n",
            "Epoch [414/1500], Step [50/148], Loss: 6253.1270\n",
            "Epoch [414/1500], Step [100/148], Loss: 3331.6785\n",
            "Epoch [415/1500], Step [50/148], Loss: 17864.0645\n",
            "Epoch [415/1500], Step [100/148], Loss: 6918.2876\n",
            "Epoch [416/1500], Step [50/148], Loss: 1762.8232\n",
            "Epoch [416/1500], Step [100/148], Loss: 1346.4359\n",
            "Epoch [417/1500], Step [50/148], Loss: 7394.8794\n",
            "Epoch [417/1500], Step [100/148], Loss: 5362.0630\n",
            "Epoch [418/1500], Step [50/148], Loss: 4656.5215\n",
            "Epoch [418/1500], Step [100/148], Loss: 3429.5193\n",
            "Epoch [419/1500], Step [50/148], Loss: 3918.6541\n",
            "Epoch [419/1500], Step [100/148], Loss: 1297.1466\n",
            "Epoch [420/1500], Step [50/148], Loss: 4631.1753\n",
            "Epoch [420/1500], Step [100/148], Loss: 11663.5273\n",
            "Epoch [421/1500], Step [50/148], Loss: 4911.9702\n",
            "Epoch [421/1500], Step [100/148], Loss: 1396.6323\n",
            "Epoch [422/1500], Step [50/148], Loss: 4038.9155\n",
            "Epoch [422/1500], Step [100/148], Loss: 4803.4111\n",
            "Epoch [423/1500], Step [50/148], Loss: 7617.6230\n",
            "Epoch [423/1500], Step [100/148], Loss: 2185.2769\n",
            "Epoch [424/1500], Step [50/148], Loss: 2465.0205\n",
            "Epoch [424/1500], Step [100/148], Loss: 2948.0735\n",
            "Epoch [425/1500], Step [50/148], Loss: 3429.9580\n",
            "Epoch [425/1500], Step [100/148], Loss: 4517.7949\n",
            "Epoch [426/1500], Step [50/148], Loss: 5051.7949\n",
            "Epoch [426/1500], Step [100/148], Loss: 3567.7244\n",
            "Epoch [427/1500], Step [50/148], Loss: 3646.2278\n",
            "Epoch [427/1500], Step [100/148], Loss: 12354.0684\n",
            "Epoch [428/1500], Step [50/148], Loss: 2061.0720\n",
            "Epoch [428/1500], Step [100/148], Loss: 4845.0977\n",
            "Epoch [429/1500], Step [50/148], Loss: 1741.2672\n",
            "Epoch [429/1500], Step [100/148], Loss: 1990.3715\n",
            "Epoch [430/1500], Step [50/148], Loss: 3938.6895\n",
            "Epoch [430/1500], Step [100/148], Loss: 2307.4360\n",
            "Epoch [431/1500], Step [50/148], Loss: 3253.4326\n",
            "Epoch [431/1500], Step [100/148], Loss: 5730.5625\n",
            "Epoch [432/1500], Step [50/148], Loss: 1501.4814\n",
            "Epoch [432/1500], Step [100/148], Loss: 2113.6062\n",
            "Epoch [433/1500], Step [50/148], Loss: 528.7579\n",
            "Epoch [433/1500], Step [100/148], Loss: 5416.4546\n",
            "Epoch [434/1500], Step [50/148], Loss: 6952.9258\n",
            "Epoch [434/1500], Step [100/148], Loss: 6336.0049\n",
            "Epoch [435/1500], Step [50/148], Loss: 6332.4023\n",
            "Epoch [435/1500], Step [100/148], Loss: 1673.0316\n",
            "Epoch [436/1500], Step [50/148], Loss: 2287.4810\n",
            "Epoch [436/1500], Step [100/148], Loss: 4948.3364\n",
            "Epoch [437/1500], Step [50/148], Loss: 4628.7651\n",
            "Epoch [437/1500], Step [100/148], Loss: 4202.2515\n",
            "Epoch [438/1500], Step [50/148], Loss: 811.5477\n",
            "Epoch [438/1500], Step [100/148], Loss: 2939.1294\n",
            "Epoch [439/1500], Step [50/148], Loss: 2535.5979\n",
            "Epoch [439/1500], Step [100/148], Loss: 1186.5789\n",
            "Epoch [440/1500], Step [50/148], Loss: 5379.2886\n",
            "Epoch [440/1500], Step [100/148], Loss: 3166.5457\n",
            "Epoch [441/1500], Step [50/148], Loss: 1854.5507\n",
            "Epoch [441/1500], Step [100/148], Loss: 3099.6660\n",
            "Epoch [442/1500], Step [50/148], Loss: 1690.6731\n",
            "Epoch [442/1500], Step [100/148], Loss: 6548.4893\n",
            "Epoch [443/1500], Step [50/148], Loss: 5498.9829\n",
            "Epoch [443/1500], Step [100/148], Loss: 1509.5312\n",
            "Epoch [444/1500], Step [50/148], Loss: 6661.3882\n",
            "Epoch [444/1500], Step [100/148], Loss: 4176.3496\n",
            "Epoch [445/1500], Step [50/148], Loss: 5101.4849\n",
            "Epoch [445/1500], Step [100/148], Loss: 1607.7878\n",
            "Epoch [446/1500], Step [50/148], Loss: 8436.6113\n",
            "Epoch [446/1500], Step [100/148], Loss: 4360.4570\n",
            "Epoch [447/1500], Step [50/148], Loss: 3084.6194\n",
            "Epoch [447/1500], Step [100/148], Loss: 4309.8398\n",
            "Epoch [448/1500], Step [50/148], Loss: 2739.8569\n",
            "Epoch [448/1500], Step [100/148], Loss: 3364.0991\n",
            "Epoch [449/1500], Step [50/148], Loss: 4578.9023\n",
            "Epoch [449/1500], Step [100/148], Loss: 2079.7507\n",
            "Epoch [450/1500], Step [50/148], Loss: 2936.1484\n",
            "Epoch [450/1500], Step [100/148], Loss: 2641.9568\n",
            "Epoch [451/1500], Step [50/148], Loss: 10517.9717\n",
            "Epoch [451/1500], Step [100/148], Loss: 6284.3438\n",
            "Epoch [452/1500], Step [50/148], Loss: 2964.6453\n",
            "Epoch [452/1500], Step [100/148], Loss: 3285.8115\n",
            "Epoch [453/1500], Step [50/148], Loss: 3345.3770\n",
            "Epoch [453/1500], Step [100/148], Loss: 3438.6179\n",
            "Epoch [454/1500], Step [50/148], Loss: 1895.7281\n",
            "Epoch [454/1500], Step [100/148], Loss: 2980.9500\n",
            "Epoch [455/1500], Step [50/148], Loss: 1735.5364\n",
            "Epoch [455/1500], Step [100/148], Loss: 2319.0642\n",
            "Epoch [456/1500], Step [50/148], Loss: 2919.0093\n",
            "Epoch [456/1500], Step [100/148], Loss: 2665.8564\n",
            "Epoch [457/1500], Step [50/148], Loss: 2409.5120\n",
            "Epoch [457/1500], Step [100/148], Loss: 1190.1099\n",
            "Epoch [458/1500], Step [50/148], Loss: 4116.2617\n",
            "Epoch [458/1500], Step [100/148], Loss: 10390.3506\n",
            "Epoch [459/1500], Step [50/148], Loss: 3444.4900\n",
            "Epoch [459/1500], Step [100/148], Loss: 1967.3593\n",
            "Epoch [460/1500], Step [50/148], Loss: 4799.7363\n",
            "Epoch [460/1500], Step [100/148], Loss: 2383.2441\n",
            "Epoch [461/1500], Step [50/148], Loss: 5137.5654\n",
            "Epoch [461/1500], Step [100/148], Loss: 647.8380\n",
            "Epoch [462/1500], Step [50/148], Loss: 4177.5371\n",
            "Epoch [462/1500], Step [100/148], Loss: 4165.2480\n",
            "Epoch [463/1500], Step [50/148], Loss: 8014.9644\n",
            "Epoch [463/1500], Step [100/148], Loss: 1935.6089\n",
            "Epoch [464/1500], Step [50/148], Loss: 2083.7976\n",
            "Epoch [464/1500], Step [100/148], Loss: 4388.8296\n",
            "Epoch [465/1500], Step [50/148], Loss: 3074.6240\n",
            "Epoch [465/1500], Step [100/148], Loss: 2419.6787\n",
            "Epoch [466/1500], Step [50/148], Loss: 3186.5747\n",
            "Epoch [466/1500], Step [100/148], Loss: 3270.7542\n",
            "Epoch [467/1500], Step [50/148], Loss: 4869.1362\n",
            "Epoch [467/1500], Step [100/148], Loss: 1923.7872\n",
            "Epoch [468/1500], Step [50/148], Loss: 36935.0586\n",
            "Epoch [468/1500], Step [100/148], Loss: 22293.2422\n",
            "Epoch [469/1500], Step [50/148], Loss: 3450.1091\n",
            "Epoch [469/1500], Step [100/148], Loss: 1369.0175\n",
            "Epoch [470/1500], Step [50/148], Loss: 2532.8921\n",
            "Epoch [470/1500], Step [100/148], Loss: 2884.9006\n",
            "Epoch [471/1500], Step [50/148], Loss: 3924.1868\n",
            "Epoch [471/1500], Step [100/148], Loss: 4803.0264\n",
            "Epoch [472/1500], Step [50/148], Loss: 4811.9980\n",
            "Epoch [472/1500], Step [100/148], Loss: 3200.2424\n",
            "Epoch [473/1500], Step [50/148], Loss: 1624.0687\n",
            "Epoch [473/1500], Step [100/148], Loss: 7916.0063\n",
            "Epoch [474/1500], Step [50/148], Loss: 1830.2314\n",
            "Epoch [474/1500], Step [100/148], Loss: 8813.4795\n",
            "Epoch [475/1500], Step [50/148], Loss: 5803.4419\n",
            "Epoch [475/1500], Step [100/148], Loss: 3951.1101\n",
            "Epoch [476/1500], Step [50/148], Loss: 4249.4502\n",
            "Epoch [476/1500], Step [100/148], Loss: 835.2137\n",
            "Epoch [477/1500], Step [50/148], Loss: 1337.2582\n",
            "Epoch [477/1500], Step [100/148], Loss: 2816.4175\n",
            "Epoch [478/1500], Step [50/148], Loss: 2607.3445\n",
            "Epoch [478/1500], Step [100/148], Loss: 4693.5273\n",
            "Epoch [479/1500], Step [50/148], Loss: 4080.9207\n",
            "Epoch [479/1500], Step [100/148], Loss: 2197.6567\n",
            "Epoch [480/1500], Step [50/148], Loss: 7790.1899\n",
            "Epoch [480/1500], Step [100/148], Loss: 10747.3848\n",
            "Epoch [481/1500], Step [50/148], Loss: 2111.6270\n",
            "Epoch [481/1500], Step [100/148], Loss: 5770.6738\n",
            "Epoch [482/1500], Step [50/148], Loss: 1837.5189\n",
            "Epoch [482/1500], Step [100/148], Loss: 6010.3486\n",
            "Epoch [483/1500], Step [50/148], Loss: 4228.0195\n",
            "Epoch [483/1500], Step [100/148], Loss: 3042.5671\n",
            "Epoch [484/1500], Step [50/148], Loss: 2666.4832\n",
            "Epoch [484/1500], Step [100/148], Loss: 2154.7900\n",
            "Epoch [485/1500], Step [50/148], Loss: 2742.1135\n",
            "Epoch [485/1500], Step [100/148], Loss: 3412.7158\n",
            "Epoch [486/1500], Step [50/148], Loss: 1778.0865\n",
            "Epoch [486/1500], Step [100/148], Loss: 4785.4434\n",
            "Epoch [487/1500], Step [50/148], Loss: 10220.3496\n",
            "Epoch [487/1500], Step [100/148], Loss: 10446.9834\n",
            "Epoch [488/1500], Step [50/148], Loss: 4008.0283\n",
            "Epoch [488/1500], Step [100/148], Loss: 13765.1201\n",
            "Epoch [489/1500], Step [50/148], Loss: 2473.5012\n",
            "Epoch [489/1500], Step [100/148], Loss: 2830.1265\n",
            "Epoch [490/1500], Step [50/148], Loss: 1593.3696\n",
            "Epoch [490/1500], Step [100/148], Loss: 1386.7688\n",
            "Epoch [491/1500], Step [50/148], Loss: 4273.5005\n",
            "Epoch [491/1500], Step [100/148], Loss: 1477.2273\n",
            "Epoch [492/1500], Step [50/148], Loss: 4417.9531\n",
            "Epoch [492/1500], Step [100/148], Loss: 2521.8662\n",
            "Epoch [493/1500], Step [50/148], Loss: 4190.9019\n",
            "Epoch [493/1500], Step [100/148], Loss: 5851.2993\n",
            "Epoch [494/1500], Step [50/148], Loss: 3104.6287\n",
            "Epoch [494/1500], Step [100/148], Loss: 3871.9858\n",
            "Epoch [495/1500], Step [50/148], Loss: 12492.3623\n",
            "Epoch [495/1500], Step [100/148], Loss: 2686.2373\n",
            "Epoch [496/1500], Step [50/148], Loss: 3774.8054\n",
            "Epoch [496/1500], Step [100/148], Loss: 2260.4607\n",
            "Epoch [497/1500], Step [50/148], Loss: 4473.9043\n",
            "Epoch [497/1500], Step [100/148], Loss: 6426.7656\n",
            "Epoch [498/1500], Step [50/148], Loss: 6213.3164\n",
            "Epoch [498/1500], Step [100/148], Loss: 8187.5967\n",
            "Epoch [499/1500], Step [50/148], Loss: 2762.4331\n",
            "Epoch [499/1500], Step [100/148], Loss: 1514.5370\n",
            "Epoch [500/1500], Step [50/148], Loss: 2046.0491\n",
            "Epoch [500/1500], Step [100/148], Loss: 1905.6256\n",
            "Epoch [501/1500], Step [50/148], Loss: 1444.9050\n",
            "Epoch [501/1500], Step [100/148], Loss: 3660.4810\n",
            "Epoch [502/1500], Step [50/148], Loss: 4303.3110\n",
            "Epoch [502/1500], Step [100/148], Loss: 6140.7612\n",
            "Epoch [503/1500], Step [50/148], Loss: 6562.6860\n",
            "Epoch [503/1500], Step [100/148], Loss: 7934.1318\n",
            "Epoch [504/1500], Step [50/148], Loss: 3060.0593\n",
            "Epoch [504/1500], Step [100/148], Loss: 3109.1985\n",
            "Epoch [505/1500], Step [50/148], Loss: 2574.9954\n",
            "Epoch [505/1500], Step [100/148], Loss: 3790.1230\n",
            "Epoch [506/1500], Step [50/148], Loss: 2771.4790\n",
            "Epoch [506/1500], Step [100/148], Loss: 3801.0669\n",
            "Epoch [507/1500], Step [50/148], Loss: 5685.4937\n",
            "Epoch [507/1500], Step [100/148], Loss: 4384.6406\n",
            "Epoch [508/1500], Step [50/148], Loss: 4212.9590\n",
            "Epoch [508/1500], Step [100/148], Loss: 1111.3687\n",
            "Epoch [509/1500], Step [50/148], Loss: 3461.1292\n",
            "Epoch [509/1500], Step [100/148], Loss: 7723.6870\n",
            "Epoch [510/1500], Step [50/148], Loss: 4180.3691\n",
            "Epoch [510/1500], Step [100/148], Loss: 3206.1018\n",
            "Epoch [511/1500], Step [50/148], Loss: 3320.7107\n",
            "Epoch [511/1500], Step [100/148], Loss: 2524.2502\n",
            "Epoch [512/1500], Step [50/148], Loss: 2433.7407\n",
            "Epoch [512/1500], Step [100/148], Loss: 2955.2949\n",
            "Epoch [513/1500], Step [50/148], Loss: 2262.4155\n",
            "Epoch [513/1500], Step [100/148], Loss: 1325.4469\n",
            "Epoch [514/1500], Step [50/148], Loss: 3736.4744\n",
            "Epoch [514/1500], Step [100/148], Loss: 1859.5369\n",
            "Epoch [515/1500], Step [50/148], Loss: 3962.8994\n",
            "Epoch [515/1500], Step [100/148], Loss: 1571.3726\n",
            "Epoch [516/1500], Step [50/148], Loss: 3031.6755\n",
            "Epoch [516/1500], Step [100/148], Loss: 2474.4385\n",
            "Epoch [517/1500], Step [50/148], Loss: 2967.3044\n",
            "Epoch [517/1500], Step [100/148], Loss: 1522.7887\n",
            "Epoch [518/1500], Step [50/148], Loss: 6037.0874\n",
            "Epoch [518/1500], Step [100/148], Loss: 3417.0427\n",
            "Epoch [519/1500], Step [50/148], Loss: 1349.7422\n",
            "Epoch [519/1500], Step [100/148], Loss: 6219.7695\n",
            "Epoch [520/1500], Step [50/148], Loss: 4572.3491\n",
            "Epoch [520/1500], Step [100/148], Loss: 3018.9365\n",
            "Epoch [521/1500], Step [50/148], Loss: 14414.6689\n",
            "Epoch [521/1500], Step [100/148], Loss: 4080.9421\n",
            "Epoch [522/1500], Step [50/148], Loss: 3692.5002\n",
            "Epoch [522/1500], Step [100/148], Loss: 3653.7007\n",
            "Epoch [523/1500], Step [50/148], Loss: 2004.1569\n",
            "Epoch [523/1500], Step [100/148], Loss: 3716.6135\n",
            "Epoch [524/1500], Step [50/148], Loss: 2558.4766\n",
            "Epoch [524/1500], Step [100/148], Loss: 5065.7402\n",
            "Epoch [525/1500], Step [50/148], Loss: 1585.8816\n",
            "Epoch [525/1500], Step [100/148], Loss: 3044.2634\n",
            "Epoch [526/1500], Step [50/148], Loss: 5289.4932\n",
            "Epoch [526/1500], Step [100/148], Loss: 4305.4556\n",
            "Epoch [527/1500], Step [50/148], Loss: 4690.2520\n",
            "Epoch [527/1500], Step [100/148], Loss: 2508.0454\n",
            "Epoch [528/1500], Step [50/148], Loss: 4319.7139\n",
            "Epoch [528/1500], Step [100/148], Loss: 1898.3475\n",
            "Epoch [529/1500], Step [50/148], Loss: 3326.2063\n",
            "Epoch [529/1500], Step [100/148], Loss: 2416.5229\n",
            "Epoch [530/1500], Step [50/148], Loss: 2313.5884\n",
            "Epoch [530/1500], Step [100/148], Loss: 2515.9243\n",
            "Epoch [531/1500], Step [50/148], Loss: 1893.5225\n",
            "Epoch [531/1500], Step [100/148], Loss: 6515.9902\n",
            "Epoch [532/1500], Step [50/148], Loss: 2868.3826\n",
            "Epoch [532/1500], Step [100/148], Loss: 1646.4734\n",
            "Epoch [533/1500], Step [50/148], Loss: 2232.4343\n",
            "Epoch [533/1500], Step [100/148], Loss: 5938.7612\n",
            "Epoch [534/1500], Step [50/148], Loss: 6975.4014\n",
            "Epoch [534/1500], Step [100/148], Loss: 3529.9734\n",
            "Epoch [535/1500], Step [50/148], Loss: 2912.1660\n",
            "Epoch [535/1500], Step [100/148], Loss: 3099.1360\n",
            "Epoch [536/1500], Step [50/148], Loss: 10376.3838\n",
            "Epoch [536/1500], Step [100/148], Loss: 2938.3992\n",
            "Epoch [537/1500], Step [50/148], Loss: 3290.6230\n",
            "Epoch [537/1500], Step [100/148], Loss: 4049.7722\n",
            "Epoch [538/1500], Step [50/148], Loss: 2983.2534\n",
            "Epoch [538/1500], Step [100/148], Loss: 3170.4109\n",
            "Epoch [539/1500], Step [50/148], Loss: 1491.4907\n",
            "Epoch [539/1500], Step [100/148], Loss: 2365.8813\n",
            "Epoch [540/1500], Step [50/148], Loss: 2175.9143\n",
            "Epoch [540/1500], Step [100/148], Loss: 2341.7563\n",
            "Epoch [541/1500], Step [50/148], Loss: 4033.9790\n",
            "Epoch [541/1500], Step [100/148], Loss: 4745.2739\n",
            "Epoch [542/1500], Step [50/148], Loss: 3452.4702\n",
            "Epoch [542/1500], Step [100/148], Loss: 1877.3853\n",
            "Epoch [543/1500], Step [50/148], Loss: 6441.4575\n",
            "Epoch [543/1500], Step [100/148], Loss: 5578.4004\n",
            "Epoch [544/1500], Step [50/148], Loss: 856.6297\n",
            "Epoch [544/1500], Step [100/148], Loss: 1596.7267\n",
            "Epoch [545/1500], Step [50/148], Loss: 3015.7458\n",
            "Epoch [545/1500], Step [100/148], Loss: 1993.0702\n",
            "Epoch [546/1500], Step [50/148], Loss: 1629.0865\n",
            "Epoch [546/1500], Step [100/148], Loss: 5616.4287\n",
            "Epoch [547/1500], Step [50/148], Loss: 1733.9103\n",
            "Epoch [547/1500], Step [100/148], Loss: 5162.2661\n",
            "Epoch [548/1500], Step [50/148], Loss: 2215.6665\n",
            "Epoch [548/1500], Step [100/148], Loss: 2310.7021\n",
            "Epoch [549/1500], Step [50/148], Loss: 3277.2156\n",
            "Epoch [549/1500], Step [100/148], Loss: 1262.2690\n",
            "Epoch [550/1500], Step [50/148], Loss: 1854.4580\n",
            "Epoch [550/1500], Step [100/148], Loss: 3130.6294\n",
            "Epoch [551/1500], Step [50/148], Loss: 3381.0969\n",
            "Epoch [551/1500], Step [100/148], Loss: 13282.7012\n",
            "Epoch [552/1500], Step [50/148], Loss: 4081.3735\n",
            "Epoch [552/1500], Step [100/148], Loss: 2121.1262\n",
            "Epoch [553/1500], Step [50/148], Loss: 7957.1650\n",
            "Epoch [553/1500], Step [100/148], Loss: 2911.3374\n",
            "Epoch [554/1500], Step [50/148], Loss: 2911.6912\n",
            "Epoch [554/1500], Step [100/148], Loss: 3144.7876\n",
            "Epoch [555/1500], Step [50/148], Loss: 8285.8447\n",
            "Epoch [555/1500], Step [100/148], Loss: 4093.8022\n",
            "Epoch [556/1500], Step [50/148], Loss: 3054.3135\n",
            "Epoch [556/1500], Step [100/148], Loss: 3767.6030\n",
            "Epoch [557/1500], Step [50/148], Loss: 3487.0657\n",
            "Epoch [557/1500], Step [100/148], Loss: 6759.7246\n",
            "Epoch [558/1500], Step [50/148], Loss: 2363.9475\n",
            "Epoch [558/1500], Step [100/148], Loss: 3072.1030\n",
            "Epoch [559/1500], Step [50/148], Loss: 2571.3540\n",
            "Epoch [559/1500], Step [100/148], Loss: 4424.8379\n",
            "Epoch [560/1500], Step [50/148], Loss: 5336.8779\n",
            "Epoch [560/1500], Step [100/148], Loss: 1706.7646\n",
            "Epoch [561/1500], Step [50/148], Loss: 1815.7529\n",
            "Epoch [561/1500], Step [100/148], Loss: 2218.4197\n",
            "Epoch [562/1500], Step [50/148], Loss: 12550.6797\n",
            "Epoch [562/1500], Step [100/148], Loss: 3859.2549\n",
            "Epoch [563/1500], Step [50/148], Loss: 1646.3832\n",
            "Epoch [563/1500], Step [100/148], Loss: 5701.5312\n",
            "Epoch [564/1500], Step [50/148], Loss: 5822.3286\n",
            "Epoch [564/1500], Step [100/148], Loss: 3422.1047\n",
            "Epoch [565/1500], Step [50/148], Loss: 1665.5978\n",
            "Epoch [565/1500], Step [100/148], Loss: 5129.2773\n",
            "Epoch [566/1500], Step [50/148], Loss: 4532.5459\n",
            "Epoch [566/1500], Step [100/148], Loss: 1978.3289\n",
            "Epoch [567/1500], Step [50/148], Loss: 6843.0449\n",
            "Epoch [567/1500], Step [100/148], Loss: 7690.0469\n",
            "Epoch [568/1500], Step [50/148], Loss: 3055.1252\n",
            "Epoch [568/1500], Step [100/148], Loss: 2186.9946\n",
            "Epoch [569/1500], Step [50/148], Loss: 944.0835\n",
            "Epoch [569/1500], Step [100/148], Loss: 2358.0703\n",
            "Epoch [570/1500], Step [50/148], Loss: 3655.8284\n",
            "Epoch [570/1500], Step [100/148], Loss: 6451.6494\n",
            "Epoch [571/1500], Step [50/148], Loss: 3192.8643\n",
            "Epoch [571/1500], Step [100/148], Loss: 2707.5723\n",
            "Epoch [572/1500], Step [50/148], Loss: 4236.4302\n",
            "Epoch [572/1500], Step [100/148], Loss: 1258.3842\n",
            "Epoch [573/1500], Step [50/148], Loss: 3073.7866\n",
            "Epoch [573/1500], Step [100/148], Loss: 10154.5771\n",
            "Epoch [574/1500], Step [50/148], Loss: 2724.1619\n",
            "Epoch [574/1500], Step [100/148], Loss: 3790.2141\n",
            "Epoch [575/1500], Step [50/148], Loss: 1437.0155\n",
            "Epoch [575/1500], Step [100/148], Loss: 3576.3301\n",
            "Epoch [576/1500], Step [50/148], Loss: 2318.3279\n",
            "Epoch [576/1500], Step [100/148], Loss: 1857.3970\n",
            "Epoch [577/1500], Step [50/148], Loss: 3826.2698\n",
            "Epoch [577/1500], Step [100/148], Loss: 2718.2046\n",
            "Epoch [578/1500], Step [50/148], Loss: 2990.2659\n",
            "Epoch [578/1500], Step [100/148], Loss: 2452.3755\n",
            "Epoch [579/1500], Step [50/148], Loss: 4148.8369\n",
            "Epoch [579/1500], Step [100/148], Loss: 8811.2744\n",
            "Epoch [580/1500], Step [50/148], Loss: 2765.7820\n",
            "Epoch [580/1500], Step [100/148], Loss: 1170.4287\n",
            "Epoch [581/1500], Step [50/148], Loss: 3215.0728\n",
            "Epoch [581/1500], Step [100/148], Loss: 1534.5548\n",
            "Epoch [582/1500], Step [50/148], Loss: 14163.0762\n",
            "Epoch [582/1500], Step [100/148], Loss: 1946.1741\n",
            "Epoch [583/1500], Step [50/148], Loss: 3387.8730\n",
            "Epoch [583/1500], Step [100/148], Loss: 1562.4069\n",
            "Epoch [584/1500], Step [50/148], Loss: 2049.1602\n",
            "Epoch [584/1500], Step [100/148], Loss: 2688.5015\n",
            "Epoch [585/1500], Step [50/148], Loss: 2793.9207\n",
            "Epoch [585/1500], Step [100/148], Loss: 4090.4199\n",
            "Epoch [586/1500], Step [50/148], Loss: 4778.1089\n",
            "Epoch [586/1500], Step [100/148], Loss: 3076.2356\n",
            "Epoch [587/1500], Step [50/148], Loss: 2412.3071\n",
            "Epoch [587/1500], Step [100/148], Loss: 5787.0688\n",
            "Epoch [588/1500], Step [50/148], Loss: 5507.0742\n",
            "Epoch [588/1500], Step [100/148], Loss: 3798.7080\n",
            "Epoch [589/1500], Step [50/148], Loss: 4201.1948\n",
            "Epoch [589/1500], Step [100/148], Loss: 5015.2412\n",
            "Epoch [590/1500], Step [50/148], Loss: 7270.8267\n",
            "Epoch [590/1500], Step [100/148], Loss: 1405.5214\n",
            "Epoch [591/1500], Step [50/148], Loss: 4527.9629\n",
            "Epoch [591/1500], Step [100/148], Loss: 1129.2914\n",
            "Epoch [592/1500], Step [50/148], Loss: 5705.2881\n",
            "Epoch [592/1500], Step [100/148], Loss: 1389.1757\n",
            "Epoch [593/1500], Step [50/148], Loss: 4427.1455\n",
            "Epoch [593/1500], Step [100/148], Loss: 6198.1602\n",
            "Epoch [594/1500], Step [50/148], Loss: 2791.6721\n",
            "Epoch [594/1500], Step [100/148], Loss: 2863.5447\n",
            "Epoch [595/1500], Step [50/148], Loss: 1711.9834\n",
            "Epoch [595/1500], Step [100/148], Loss: 1028.5383\n",
            "Epoch [596/1500], Step [50/148], Loss: 1246.1831\n",
            "Epoch [596/1500], Step [100/148], Loss: 1295.0175\n",
            "Epoch [597/1500], Step [50/148], Loss: 2290.0227\n",
            "Epoch [597/1500], Step [100/148], Loss: 1477.0085\n",
            "Epoch [598/1500], Step [50/148], Loss: 1216.6438\n",
            "Epoch [598/1500], Step [100/148], Loss: 1808.7434\n",
            "Epoch [599/1500], Step [50/148], Loss: 4153.6255\n",
            "Epoch [599/1500], Step [100/148], Loss: 3097.1191\n",
            "Epoch [600/1500], Step [50/148], Loss: 3263.9116\n",
            "Epoch [600/1500], Step [100/148], Loss: 5202.0977\n",
            "Epoch [601/1500], Step [50/148], Loss: 4100.9473\n",
            "Epoch [601/1500], Step [100/148], Loss: 2563.0891\n",
            "Epoch [602/1500], Step [50/148], Loss: 5072.0103\n",
            "Epoch [602/1500], Step [100/148], Loss: 1228.2759\n",
            "Epoch [603/1500], Step [50/148], Loss: 8717.8223\n",
            "Epoch [603/1500], Step [100/148], Loss: 9436.3848\n",
            "Epoch [604/1500], Step [50/148], Loss: 5071.0264\n",
            "Epoch [604/1500], Step [100/148], Loss: 2840.1499\n",
            "Epoch [605/1500], Step [50/148], Loss: 3055.1477\n",
            "Epoch [605/1500], Step [100/148], Loss: 2864.7976\n",
            "Epoch [606/1500], Step [50/148], Loss: 2890.9243\n",
            "Epoch [606/1500], Step [100/148], Loss: 3487.6221\n",
            "Epoch [607/1500], Step [50/148], Loss: 1454.1896\n",
            "Epoch [607/1500], Step [100/148], Loss: 1527.2806\n",
            "Epoch [608/1500], Step [50/148], Loss: 6335.8813\n",
            "Epoch [608/1500], Step [100/148], Loss: 9284.0156\n",
            "Epoch [609/1500], Step [50/148], Loss: 6573.9238\n",
            "Epoch [609/1500], Step [100/148], Loss: 4479.4023\n",
            "Epoch [610/1500], Step [50/148], Loss: 1391.9916\n",
            "Epoch [610/1500], Step [100/148], Loss: 2862.0007\n",
            "Epoch [611/1500], Step [50/148], Loss: 3579.9077\n",
            "Epoch [611/1500], Step [100/148], Loss: 2772.4082\n",
            "Epoch [612/1500], Step [50/148], Loss: 3460.8784\n",
            "Epoch [612/1500], Step [100/148], Loss: 3767.7847\n",
            "Epoch [613/1500], Step [50/148], Loss: 2487.8533\n",
            "Epoch [613/1500], Step [100/148], Loss: 3990.8857\n",
            "Epoch [614/1500], Step [50/148], Loss: 2859.1748\n",
            "Epoch [614/1500], Step [100/148], Loss: 2966.3794\n",
            "Epoch [615/1500], Step [50/148], Loss: 11688.5859\n",
            "Epoch [615/1500], Step [100/148], Loss: 1921.8527\n",
            "Epoch [616/1500], Step [50/148], Loss: 6071.8281\n",
            "Epoch [616/1500], Step [100/148], Loss: 4491.9395\n",
            "Epoch [617/1500], Step [50/148], Loss: 3837.8540\n",
            "Epoch [617/1500], Step [100/148], Loss: 3841.9397\n",
            "Epoch [618/1500], Step [50/148], Loss: 4299.3901\n",
            "Epoch [618/1500], Step [100/148], Loss: 2087.4802\n",
            "Epoch [619/1500], Step [50/148], Loss: 5681.6382\n",
            "Epoch [619/1500], Step [100/148], Loss: 2486.5664\n",
            "Epoch [620/1500], Step [50/148], Loss: 2664.8906\n",
            "Epoch [620/1500], Step [100/148], Loss: 1269.3805\n",
            "Epoch [621/1500], Step [50/148], Loss: 3910.0305\n",
            "Epoch [621/1500], Step [100/148], Loss: 4450.3276\n",
            "Epoch [622/1500], Step [50/148], Loss: 8367.3467\n",
            "Epoch [622/1500], Step [100/148], Loss: 2181.8191\n",
            "Epoch [623/1500], Step [50/148], Loss: 3826.3049\n",
            "Epoch [623/1500], Step [100/148], Loss: 3462.5056\n",
            "Epoch [624/1500], Step [50/148], Loss: 1739.0272\n",
            "Epoch [624/1500], Step [100/148], Loss: 7377.3452\n",
            "Epoch [625/1500], Step [50/148], Loss: 1097.0609\n",
            "Epoch [625/1500], Step [100/148], Loss: 7497.1050\n",
            "Epoch [626/1500], Step [50/148], Loss: 5058.6973\n",
            "Epoch [626/1500], Step [100/148], Loss: 2919.6716\n",
            "Epoch [627/1500], Step [50/148], Loss: 6225.7256\n",
            "Epoch [627/1500], Step [100/148], Loss: 3084.9856\n",
            "Epoch [628/1500], Step [50/148], Loss: 4436.8745\n",
            "Epoch [628/1500], Step [100/148], Loss: 4014.0559\n",
            "Epoch [629/1500], Step [50/148], Loss: 1084.0511\n",
            "Epoch [629/1500], Step [100/148], Loss: 6268.6494\n",
            "Epoch [630/1500], Step [50/148], Loss: 4408.1226\n",
            "Epoch [630/1500], Step [100/148], Loss: 4144.7554\n",
            "Epoch [631/1500], Step [50/148], Loss: 7552.7412\n",
            "Epoch [631/1500], Step [100/148], Loss: 2166.0010\n",
            "Epoch [632/1500], Step [50/148], Loss: 5564.3730\n",
            "Epoch [632/1500], Step [100/148], Loss: 6430.3306\n",
            "Epoch [633/1500], Step [50/148], Loss: 4808.9707\n",
            "Epoch [633/1500], Step [100/148], Loss: 1065.9283\n",
            "Epoch [634/1500], Step [50/148], Loss: 1999.5531\n",
            "Epoch [634/1500], Step [100/148], Loss: 4989.7603\n",
            "Epoch [635/1500], Step [50/148], Loss: 3964.8503\n",
            "Epoch [635/1500], Step [100/148], Loss: 1869.8622\n",
            "Epoch [636/1500], Step [50/148], Loss: 2853.6279\n",
            "Epoch [636/1500], Step [100/148], Loss: 2573.7620\n",
            "Epoch [637/1500], Step [50/148], Loss: 2970.6665\n",
            "Epoch [637/1500], Step [100/148], Loss: 2987.5276\n",
            "Epoch [638/1500], Step [50/148], Loss: 3600.7822\n",
            "Epoch [638/1500], Step [100/148], Loss: 2860.6343\n",
            "Epoch [639/1500], Step [50/148], Loss: 10687.4922\n",
            "Epoch [639/1500], Step [100/148], Loss: 6470.8638\n",
            "Epoch [640/1500], Step [50/148], Loss: 6137.3901\n",
            "Epoch [640/1500], Step [100/148], Loss: 2097.7842\n",
            "Epoch [641/1500], Step [50/148], Loss: 2732.8860\n",
            "Epoch [641/1500], Step [100/148], Loss: 5372.5312\n",
            "Epoch [642/1500], Step [50/148], Loss: 2760.2734\n",
            "Epoch [642/1500], Step [100/148], Loss: 10695.7334\n",
            "Epoch [643/1500], Step [50/148], Loss: 4573.4810\n",
            "Epoch [643/1500], Step [100/148], Loss: 2767.3328\n",
            "Epoch [644/1500], Step [50/148], Loss: 3170.5305\n",
            "Epoch [644/1500], Step [100/148], Loss: 1483.0708\n",
            "Epoch [645/1500], Step [50/148], Loss: 3688.3096\n",
            "Epoch [645/1500], Step [100/148], Loss: 3282.7878\n",
            "Epoch [646/1500], Step [50/148], Loss: 934.9494\n",
            "Epoch [646/1500], Step [100/148], Loss: 2153.9873\n",
            "Epoch [647/1500], Step [50/148], Loss: 13243.4912\n",
            "Epoch [647/1500], Step [100/148], Loss: 4289.6060\n",
            "Epoch [648/1500], Step [50/148], Loss: 5037.5435\n",
            "Epoch [648/1500], Step [100/148], Loss: 2646.1309\n",
            "Epoch [649/1500], Step [50/148], Loss: 1543.3458\n",
            "Epoch [649/1500], Step [100/148], Loss: 5647.6899\n",
            "Epoch [650/1500], Step [50/148], Loss: 2569.8552\n",
            "Epoch [650/1500], Step [100/148], Loss: 5009.2329\n",
            "Epoch [651/1500], Step [50/148], Loss: 3517.3572\n",
            "Epoch [651/1500], Step [100/148], Loss: 4306.8579\n",
            "Epoch [652/1500], Step [50/148], Loss: 2578.2346\n",
            "Epoch [652/1500], Step [100/148], Loss: 3442.7654\n",
            "Epoch [653/1500], Step [50/148], Loss: 3215.5007\n",
            "Epoch [653/1500], Step [100/148], Loss: 1361.5413\n",
            "Epoch [654/1500], Step [50/148], Loss: 2247.7402\n",
            "Epoch [654/1500], Step [100/148], Loss: 1645.6813\n",
            "Epoch [655/1500], Step [50/148], Loss: 4800.8960\n",
            "Epoch [655/1500], Step [100/148], Loss: 2299.2905\n",
            "Epoch [656/1500], Step [50/148], Loss: 3333.7390\n",
            "Epoch [656/1500], Step [100/148], Loss: 2720.4319\n",
            "Epoch [657/1500], Step [50/148], Loss: 6154.4463\n",
            "Epoch [657/1500], Step [100/148], Loss: 2163.4639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paPqLx9o7aB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}